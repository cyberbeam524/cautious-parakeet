{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark Processing Script](#Create-PySpark-Processing-Script)\n",
    "1. [Run SageMaker Processing Job](#Run-SageMaker-Processing-Job)\n",
    "1. [Explore Aggregated Features](#Explore-Aggregated-Features)\n",
    "1. [Validate Feature Group for Records](#Validate-Feature-Group-for-Records)\n",
    "\n",
    "**Recommended settings to run this notebook in SageMaker Studio:**\n",
    "\n",
    "- Image: Data Science\n",
    "- Kernel: Python3\n",
    "- Instance type: <font color='blue'>ml.m5.large (2 vCPU + 8 GiB)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "- This notebook takes raw credit card transactions data (csv) generated by \n",
    "[notebook 0](./0_prepare_transactions_dataset.ipynb) and aggregates the raw features to create new features (ratios) via <b>SageMaker Processing</b> PySpark Job. These aggregated features alongside the raw original features will be leveraged in the training phase of a Credit Card Fraud Detection model in the next step (see notebook [notebook 3](./3_train_and_deploy_model.ipynb)).\n",
    "\n",
    "- As part of the Spark job, we also select the latest weekly aggregated features - `num_trans_last_1w` and `avg_amt_last_1w` grouped by `cc_num` (credit card number) and populate these features into the <b>SageMaker Online Feature Store</b> as a feature group. This feature group (`cc-agg-batch-fg`) was created in notebook [notebook 1](./1_setup.ipynb).\n",
    "\n",
    "- [Amazon SageMaker Processing](https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-sagemaker-processing-now-supports-built-in-spark-containers-for-big-data-processing/) lets customers run analytics jobs for data engineering and model evaluation on Amazon SageMaker easily and at scale. It provides a fully managed Spark environment for data processing or feature engineering workloads.\n",
    "\n",
    "![SegmentLocal](images/batch_ingestion.png \"connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import random\n",
    "import boto3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.240.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Batch Aggregation using SageMaker PySpark Processing Job]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 06:27:19] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> <span style=\"font-weight: bold\">[</span>Batch Aggregation using SageMaker PySpark Processing Job<span style=\"font-weight: bold\">]</span>              <a href=\"file:///tmp/ipykernel_8028/321880227.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">321880227.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_8028/321880227.py#1\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 06:27:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m \u001b[1m[\u001b[0mBatch Aggregation using SageMaker PySpark Processing Job\u001b[1m]\u001b[0m              \u001b]8;id=539972;file:///tmp/ipykernel_8028/321880227.py\u001b\\\u001b[2m321880227.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=522587;file:///tmp/ipykernel_8028/321880227.py#1\u001b\\\u001b[2m1\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info('[Batch Aggregation using SageMaker PySpark Processing Job]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "INPUT_KEY_PREFIX = 'raw_clicks'\n",
    "OUTPUT_KEY_PREFIX = 'aggregated_clicks'\n",
    "LOCAL_DIR = '/home/sagemaker-user/cautious-parakeet/notebooks/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to SageMaker Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/cautious-parakeet/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv to ./clickstream.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://{BUCKET}/{INPUT_KEY_PREFIX}/clickstream.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"s3://{BUCKET}/{INPUT_KEY_PREFIX}/clickstream.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['event_id', 'timestamp', 'customer_id', 'session_id', 'event_type',\n",
      "       'product_id', 'product_category', 'price', 'order_in_session',\n",
      "       'purchased_items', 'total_amount', 'interaction_value',\n",
      "       'cumsum_interactions'],\n",
      "      dtype='object')\n",
      "TOTALLENGTH: 5211\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>price</th>\n",
       "      <th>order_in_session</th>\n",
       "      <th>purchased_items</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>interaction_value</th>\n",
       "      <th>cumsum_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97ae495e-96b2-452b-b4e1-40e121984851</td>\n",
       "      <td>2025-03-08 05:16:50</td>\n",
       "      <td>43</td>\n",
       "      <td>c8c97aa2-9314-426d-b623-c2322e02c919</td>\n",
       "      <td>page_view</td>\n",
       "      <td>375.0</td>\n",
       "      <td>books</td>\n",
       "      <td>148.45</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6da2985f-7883-4697-8f8a-d3c620615825</td>\n",
       "      <td>2025-03-08 05:16:55</td>\n",
       "      <td>43</td>\n",
       "      <td>c8c97aa2-9314-426d-b623-c2322e02c919</td>\n",
       "      <td>click</td>\n",
       "      <td>375.0</td>\n",
       "      <td>books</td>\n",
       "      <td>22.72</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06a88c47-3235-4bc1-b5c4-c26765259034</td>\n",
       "      <td>2025-03-08 05:16:59</td>\n",
       "      <td>43</td>\n",
       "      <td>c8c97aa2-9314-426d-b623-c2322e02c919</td>\n",
       "      <td>page_view</td>\n",
       "      <td>375.0</td>\n",
       "      <td>books</td>\n",
       "      <td>213.00</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>914c8a91-5cba-4e61-b514-de4a9dc9a6ce</td>\n",
       "      <td>2025-03-08 05:17:08</td>\n",
       "      <td>43</td>\n",
       "      <td>c8c97aa2-9314-426d-b623-c2322e02c919</td>\n",
       "      <td>scroll</td>\n",
       "      <td>375.0</td>\n",
       "      <td>books</td>\n",
       "      <td>284.64</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5285faef-9425-4506-b744-5ad33f75eae9</td>\n",
       "      <td>2025-03-08 05:17:17</td>\n",
       "      <td>43</td>\n",
       "      <td>c8c97aa2-9314-426d-b623-c2322e02c919</td>\n",
       "      <td>scroll</td>\n",
       "      <td>375.0</td>\n",
       "      <td>books</td>\n",
       "      <td>435.54</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               event_id            timestamp  customer_id  \\\n",
       "0  97ae495e-96b2-452b-b4e1-40e121984851  2025-03-08 05:16:50           43   \n",
       "1  6da2985f-7883-4697-8f8a-d3c620615825  2025-03-08 05:16:55           43   \n",
       "2  06a88c47-3235-4bc1-b5c4-c26765259034  2025-03-08 05:16:59           43   \n",
       "3  914c8a91-5cba-4e61-b514-de4a9dc9a6ce  2025-03-08 05:17:08           43   \n",
       "4  5285faef-9425-4506-b744-5ad33f75eae9  2025-03-08 05:17:17           43   \n",
       "\n",
       "                             session_id event_type  product_id  \\\n",
       "0  c8c97aa2-9314-426d-b623-c2322e02c919  page_view       375.0   \n",
       "1  c8c97aa2-9314-426d-b623-c2322e02c919      click       375.0   \n",
       "2  c8c97aa2-9314-426d-b623-c2322e02c919  page_view       375.0   \n",
       "3  c8c97aa2-9314-426d-b623-c2322e02c919     scroll       375.0   \n",
       "4  c8c97aa2-9314-426d-b623-c2322e02c919     scroll       375.0   \n",
       "\n",
       "  product_category   price  order_in_session purchased_items  total_amount  \\\n",
       "0            books  148.45                 1             NaN           NaN   \n",
       "1            books   22.72                 2             NaN           NaN   \n",
       "2            books  213.00                 3             NaN           NaN   \n",
       "3            books  284.64                 4             NaN           NaN   \n",
       "4            books  435.54                 5             NaN           NaN   \n",
       "\n",
       "   interaction_value  cumsum_interactions  \n",
       "0                  4                    4  \n",
       "1                  1                    5  \n",
       "2                  4                    9  \n",
       "3                  2                   11  \n",
       "4                  2                   13  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickstread_df = pd.read_csv(f\"./clickstream.csv\")\n",
    "# set(clickstread_df[\"event_type\"])\n",
    "# Index(['Unnamed: 0', 'event_id', 'timestamp', 'customer_id', 'session_id',\n",
    "#        'event_type', 'product_id', 'product_category', 'price',\n",
    "#        'order_in_session', 'purchased_items', 'total_amount',\n",
    "#        'interaction_value', 'cumsum_interactions'],\n",
    "#       dtype='object')\n",
    "print(clickstread_df.columns)\n",
    "print(f\"TOTALLENGTH: {len(clickstread_df)}\")\n",
    "clickstread_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_aggregation_user_behavior.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_aggregation_user_behavior.py\n",
    "from pyspark.sql.types import (StructField, StructType, StringType, DoubleType, \n",
    "                               TimestampType, LongType)\n",
    "from pyspark.sql.functions import desc, dense_rank, col, when, count, avg, lower, trim\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Total unique users expected (for validation)\n",
    "TOTAL_UNIQUE_USERS = 10000\n",
    "# Replace with your actual feature group name for batch aggregated user behavior features\n",
    "# aggregate_feature_group_name: cc-agg-fg\n",
    "# aggregate_batch_feature_group_name: cc-agg-batch-fg\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "feature_store_client = boto3.client(service_name='sagemaker-featurestore-runtime')\n",
    "\n",
    "def parse_args() -> Namespace:\n",
    "    parser = ArgumentParser(description='Spark Job Input and Output Args for User Behavior Aggregation')\n",
    "    parser.add_argument('--s3_input_bucket', type=str, help='S3 Input Bucket')\n",
    "    parser.add_argument('--s3_input_key_prefix', type=str, help='S3 Input Key Prefix')\n",
    "    parser.add_argument('--s3_output_bucket', type=str, help='S3 Output Bucket')\n",
    "    parser.add_argument('--s3_output_key_prefix', type=str, help='S3 Output Key Prefix')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def define_schema() -> StructType:\n",
    "    # Define schema based on your sample CSV:\n",
    "    # ['event_id', 'timestamp', 'customer_id', 'session_id', 'event_type',\n",
    "    #    'product_id', 'product_category', 'price', 'order_in_session',\n",
    "    #    'purchased_items', 'total_amount', 'interaction_value',\n",
    "    #    'cumsum_interactions']%%!\n",
    "    \n",
    "    schema = StructType([\n",
    "        # StructField('index', StringType(), True),  # if present as the first column\n",
    "        StructField('event_id', StringType(), True),\n",
    "        StructField('timestamp', TimestampType(), True),\n",
    "        StructField('customer_id', LongType(), True),\n",
    "        StructField('session_id', StringType(), True),\n",
    "        StructField('event_type', StringType(), True),\n",
    "        StructField('product_id', DoubleType(), True),\n",
    "        StructField('product_category', StringType(), True),\n",
    "        StructField('price', DoubleType(), True),\n",
    "        StructField('order_in_session', LongType(), True),\n",
    "        # These fields may be null if not applicable; adjust types as needed:\n",
    "        StructField('purchased_items', LongType(), True),\n",
    "        StructField('total_amount', DoubleType(), True),\n",
    "        StructField('interaction_value', LongType(), True),\n",
    "        StructField('cumsum_interactions', LongType(), True)\n",
    "    ])\n",
    "    return schema\n",
    "\n",
    "def aggregate_features(args: Namespace, schema: StructType, spark: SparkSession) -> DataFrame:\n",
    "    logger.info('[Loggs][Read User Behavior Data as Spark DataFrame]')\n",
    "    input_path = f's3a://{os.path.join(args.s3_input_bucket, args.s3_input_key_prefix)}'\n",
    "    events_df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "\n",
    "    events_df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "    distinct_events = events_df.select('event_type').distinct().collect()\n",
    "    events_df.printSchema()\n",
    "    logger.info(f\"[Loggs] Total rows in input: {events_df.count()} --- {distinct_events}\")\n",
    "    \n",
    "    logger.info('[Loggs][Filter and Aggregate Order Data]')\n",
    "    # We assume that order events are indicated by event_type = 'order'\n",
    "    # If your data marks orders in another way (e.g., non-null total_amount), adjust the filter accordingly.\n",
    "    # orders_df = events_df.filter(col('event_type') == 'purchase')\n",
    "    orders_df = events_df.filter(lower(trim(col('event_type'))) == 'purchase')\n",
    "\n",
    "    logger.info(f\"[Loggs]Order events count: {orders_df.count()}\")\n",
    "    \n",
    "    # Define windows:\n",
    "    # For batch aggregates, we compute aggregates over the past 1 week.\n",
    "    window_1w = Window.partitionBy('customer_id')\\\n",
    "                      .orderBy(col('timestamp').cast(\"long\"))\\\n",
    "                      .rangeBetween(-7 * 24 * 3600, 0)\n",
    "    \n",
    "    aggregated_df = orders_df.withColumn('total_orders_last_1w', count('*').over(window_1w)) \\\n",
    "                              .withColumn('avg_order_value_last_1w', avg(col('total_amount')).over(window_1w))\n",
    "    \n",
    "    # Optionally, you can compute additional features. For example, if you want to include a real-time like metric\n",
    "    # computed over a shorter interval (e.g., last 5 minutes) for clicks:\n",
    "    clicks_df = events_df.filter(col('event_type') == 'click')\n",
    "\n",
    "    clicks_df = events_df.filter(col('event_type') == 'click')\n",
    "    logger.info(f\"[Loggs]Click events count: {clicks_df.count()}\")\n",
    "\n",
    "    window_5m = Window.partitionBy('customer_id')\\\n",
    "                      .orderBy(col('timestamp').cast(\"long\"))\\\n",
    "                      .rangeBetween(-5 * 60, 0)\n",
    "    clicks_agg = clicks_df.withColumn('clicks_last_5m', count('*').over(window_5m))\n",
    "    \n",
    "    # Join the aggregates by customer_id. Depending on your needs, you may join batch and streaming features,\n",
    "    # or process them in separate feature groups.\n",
    "    agg_joined = aggregated_df.join(clicks_agg.select('customer_id', 'clicks_last_5m'),\n",
    "                                    on='customer_id', how='left')\n",
    "    \n",
    "    # Remove duplicates by taking the latest record per customer.\n",
    "    window_latest = Window.partitionBy('customer_id').orderBy(desc('timestamp'))\n",
    "    sorted_df = agg_joined.withColumn('rank', dense_rank().over(window_latest))\n",
    "    grouped_df = sorted_df.filter(col('rank') == 1).drop('rank')\n",
    "    logger.info(f\"[Loggs]aggregate_features - grouped_df count: {grouped_df.count()}\")\n",
    "    # Select the fields that match your feature group schema\n",
    "    # Here, we assume the batch feature group expects: user_id, total_orders_last_1w, avg_order_value_last_1w, clicks_last_5m, event_time\n",
    "    final_df = grouped_df.select(\n",
    "        col('customer_id').alias('user_id'),\n",
    "        col('total_orders_last_1w'),\n",
    "        col('avg_order_value_last_1w'),\n",
    "        col('clicks_last_5m'),\n",
    "        col('timestamp').alias('event_time')\n",
    "    )\n",
    "    return final_df\n",
    "\n",
    "def write_to_s3(args: Namespace, aggregated_features: DataFrame) -> None:\n",
    "    logger.info(f'[Loggs][Write Aggregated Features to S3] {aggregated_features.count()}')\n",
    "    output_path = 's3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix)\n",
    "    aggregated_features.coalesce(1) \\\n",
    "                       .write.format('csv') \\\n",
    "                       .option('header', True) \\\n",
    "                       .mode('overwrite') \\\n",
    "                       .save(output_path)\n",
    "    \n",
    "def group_by_user(aggregated_features: DataFrame) -> DataFrame: \n",
    "    logger.info('[Loggs][Group Aggregated Features by User]')\n",
    "    window = Window.partitionBy('user_id').orderBy(desc('event_time'))\n",
    "    sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "    logger.info(f'[Loggs]sorted_df count {sorted_df.count()}')\n",
    "    grouped_df = sorted_df.filter(col('rank') == 1).drop('rank')\n",
    "    logger.info(f'[Loggs]grouped_df count {grouped_df.count()}')\n",
    "    # Select only the columns to be ingested into the feature store\n",
    "    sliced_df = grouped_df.select('user_id', 'total_orders_last_1w', 'avg_order_value_last_1w', 'clicks_last_5m')\n",
    "    logger.info(f'[Loggs]sliced_df count {sliced_df.count()}')\n",
    "    return sliced_df\n",
    "\n",
    "def transform_row(sliced_df: DataFrame) -> list:\n",
    "    logger.info('[Loggs][Transform Spark DataFrame Rows to Feature Store Records]')\n",
    "    records = []\n",
    "    for row in sliced_df.rdd.collect():\n",
    "        record = []\n",
    "        user_id, total_orders_last_1w, avg_order_value_last_1w, clicks_last_5m = row\n",
    "        if user_id is not None:\n",
    "            record.append({'FeatureName': 'user_id', 'ValueAsString': str(user_id)})\n",
    "            record.append({'FeatureName': 'total_orders_last_1w', 'ValueAsString': str(total_orders_last_1w)})\n",
    "            record.append({'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': str(round(avg_order_value_last_1w, 2) if avg_order_value_last_1w else 0.0)})\n",
    "            # record.append({'FeatureName': 'clicks_last_5m', 'ValueAsString': str(clicks_last_5m if clicks_last_5m else 0)})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def write_to_feature_store(records: list) -> None:\n",
    "    logger.info(f'[Loggs][Write Grouped Features to SageMaker Feature Store] records count- {len(records)}')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'event_time',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "        logger.info('[Loggs]Success = {}, Failed = {}'.format(success, fail))\n",
    "    logger.info('[Loggs]Success = {}'.format(success))\n",
    "    logger.info('[Loggs]Fail = {}'.format(fail))\n",
    "    # You can adjust these assertions based on your expected unique user count\n",
    "    assert success <= TOTAL_UNIQUE_USERS\n",
    "    assert fail == 0\n",
    "\n",
    "def run_spark_job():\n",
    "    spark = SparkSession.builder.appName('UserBehaviorBatchAggregationJob5').getOrCreate()\n",
    "    args = parse_args()\n",
    "    schema = define_schema()\n",
    "    aggregated_features = aggregate_features(args, schema, spark)\n",
    "    write_to_s3(args, aggregated_features)\n",
    "    grouped_features = group_by_user(aggregated_features)\n",
    "    records = transform_row(grouped_features)\n",
    "    write_to_feature_store(records)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run_spark_job()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Read in the file\n",
    "# with open('batch_aggregation_temp.py', 'r') as file :\n",
    "#   filedata = file.read()\n",
    "\n",
    "# # Replace the target string\n",
    "# filedata = filedata.replace('|agg_batch_fg_name|', aggregate_batch_feature_group_name)\n",
    "\n",
    "# # Write the file out again\n",
    "# with open('batch_aggregation.py', 'w') as file:\n",
    "#   file.write(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(base_job_name='sagemaker-processing', \n",
    "                                   framework_version='2.4', # spark version\n",
    "                                   role=sagemaker_role, \n",
    "                                   # instance_count=1, \n",
    "                                   # instance_type='ml.m5.4xlarge', \n",
    "                                   instance_count=2, \n",
    "                                   instance_type='ml.t3.xlarge', \n",
    "                                   env={'AWS_DEFAULT_REGION': boto3.Session().region_name},\n",
    "                                   max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "spark_processor.run(submit_app='batch_aggregation_user_behavior.py', \n",
    "                    arguments=['--s3_input_bucket', BUCKET, \n",
    "                               '--s3_input_key_prefix', INPUT_KEY_PREFIX, \n",
    "                               '--s3_output_bucket', BUCKET, \n",
    "                               '--s3_output_key_prefix', OUTPUT_KEY_PREFIX],\n",
    "                    spark_event_logs_s3_uri='s3://{}/logs'.format(BUCKET),\n",
    "                    logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Explore Aggregated Features \n",
    "<p> The SageMaker Processing Job above creates the aggregated features alongside the raw features and writes it to S3. \n",
    "Let us verify this output using the code below and prep it to be used in the next step for model training.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Copy results csv from S3 to local directory. Below step may fail if you are running the notebook for the first time. Hence any file missing error could be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/home/sagemaker-user/cautious-parakeet/notebooks/data/aggregated_clicks/part*.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-ap-southeast-1-850995562355/aggregated_clicks/part-00000-2373ee67-04f3-4579-886a-12b7a977fa39-c000.csv to data/aggregated_clicks/part-00000-2373ee67-04f3-4579-886a-12b7a977fa39-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://{BUCKET}/{OUTPUT_KEY_PREFIX}/ {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/ --recursive --exclude '_SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp s3://sagemaker-ap-southeast-1-850995562355/aggregated/ {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/cc/ --recursive --exclude '_SUCCESS'\n",
    "# !mv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/cc/part*.csv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/partcc.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8028/1654825752.py:8: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  agg_features['event_time'] = agg_features['event_time'].values.astype('datetime64[ns]')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>total_orders_last_1w</th>\n",
       "      <th>avg_order_value_last_1w</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.320007</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.320007</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.320007</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.320007</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.320007</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  total_orders_last_1w  avg_order_value_last_1w          event_time\n",
       "0       94                     1               307.320007 2025-03-01 23:28:53\n",
       "1       94                     1               307.320007 2025-03-01 23:28:53\n",
       "2       94                     1               307.320007 2025-03-01 23:28:53\n",
       "3       94                     1               307.320007 2025-03-01 23:28:53\n",
       "4       94                     1               307.320007 2025-03-01 23:28:53"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_features = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv')\n",
    "# agg_features = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/partcc.csv')\n",
    "agg_features.dropna(inplace=True)\n",
    "agg_features = agg_features.drop(columns = [\"clicks_last_5m\"])\n",
    "agg_features['total_orders_last_1w'] = agg_features['total_orders_last_1w'].astype(np.int64)\n",
    "agg_features['avg_order_value_last_1w'] = agg_features['avg_order_value_last_1w'].astype(np.float32)\n",
    "# agg_features['event_time'] = agg_features['event_time'].astype(np.datetime64)\n",
    "agg_features['event_time'] = agg_features['event_time'].values.astype('datetime64[ns]')\n",
    "agg_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_features.to_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Remove the intermediate `part.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Validate Feature Group for Records\n",
    "Let's randomly pick N credit card numbers from the `processing_output.csv` and verify if records exist in the feature group `<aggregate_batch_feature_group_name>` for these card numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 3 # number of random records to validate\n",
    "FEATURE_GROUP = aggregate_batch_feature_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>total_orders_last_1w</th>\n",
       "      <th>avg_order_value_last_1w</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.32</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.32</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.32</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.32</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>307.32</td>\n",
       "      <td>2025-03-01 23:28:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  total_orders_last_1w  avg_order_value_last_1w           event_time\n",
       "0       94                     1                   307.32  2025-03-01 23:28:53\n",
       "1       94                     1                   307.32  2025-03-01 23:28:53\n",
       "2       94                     1                   307.32  2025-03-01 23:28:53\n",
       "3       94                     1                   307.32  2025-03-01 23:28:53\n",
       "4       94                     1                   307.32  2025-03-01 23:28:53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_out_df = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv', nrows=10000)\n",
    "processing_out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[915, 771, 929]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_nums = random.sample(processing_out_df['user_id'].tolist(), N)\n",
    "cc_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Using SageMaker Feature Store run-time client, we can verify if records exist in the feature group for the picked `cc_nums` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_store_client = boto3.Session().client(service_name='sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FeatureName': 'user_id', 'ValueAsString': '915'}, {'FeatureName': 'total_orders_last_1w', 'ValueAsString': '1'}, {'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': '425.39'}, {'FeatureName': 'event_time', 'ValueAsString': '1741501126'}]\n",
      "[{'FeatureName': 'user_id', 'ValueAsString': '771'}, {'FeatureName': 'total_orders_last_1w', 'ValueAsString': '1'}, {'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': '340.74'}, {'FeatureName': 'event_time', 'ValueAsString': '1741501129'}]\n",
      "[{'FeatureName': 'user_id', 'ValueAsString': '929'}, {'FeatureName': 'total_orders_last_1w', 'ValueAsString': '1'}, {'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': '75.05'}, {'FeatureName': 'event_time', 'ValueAsString': '1741501128'}]\n"
     ]
    }
   ],
   "source": [
    "success, fail = 0, 0\n",
    "for cc_num in cc_nums:\n",
    "    response = feature_store_client.get_record(FeatureGroupName=FEATURE_GROUP, \n",
    "                                               RecordIdentifierValueAsString=str(cc_num))\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] == 200 and 'Record' in response.keys():\n",
    "        success += 1\n",
    "        print(response['Record'])\n",
    "    else:\n",
    "        print(response)\n",
    "        fail += 1\n",
    "assert success == N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
