{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark Processing Script](#Create-PySpark-Processing-Script)\n",
    "1. [Run SageMaker Processing Job](#Run-SageMaker-Processing-Job)\n",
    "1. [Explore Aggregated Features](#Explore-Aggregated-Features)\n",
    "1. [Validate Feature Group for Records](#Validate-Feature-Group-for-Records)\n",
    "\n",
    "**Recommended settings to run this notebook in SageMaker Studio:**\n",
    "\n",
    "- Image: Data Science\n",
    "- Kernel: Python3\n",
    "- Instance type: <font color='blue'>ml.m5.large (2 vCPU + 8 GiB)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "- This notebook takes raw credit card transactions data (csv) generated by \n",
    "[notebook 0](./0_prepare_transactions_dataset.ipynb) and aggregates the raw features to create new features (ratios) via <b>SageMaker Processing</b> PySpark Job. These aggregated features alongside the raw original features will be leveraged in the training phase of a Credit Card Fraud Detection model in the next step (see notebook [notebook 3](./3_train_and_deploy_model.ipynb)).\n",
    "\n",
    "- As part of the Spark job, we also select the latest weekly aggregated features - `num_trans_last_1w` and `avg_amt_last_1w` grouped by `cc_num` (credit card number) and populate these features into the <b>SageMaker Online Feature Store</b> as a feature group. This feature group (`cc-agg-batch-fg`) was created in notebook [notebook 1](./1_setup.ipynb).\n",
    "\n",
    "- [Amazon SageMaker Processing](https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-sagemaker-processing-now-supports-built-in-spark-containers-for-big-data-processing/) lets customers run analytics jobs for data engineering and model evaluation on Amazon SageMaker easily and at scale. It provides a fully managed Spark environment for data processing or feature engineering workloads.\n",
    "\n",
    "![SegmentLocal](images/batch_ingestion.png \"connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import random\n",
    "import boto3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.240.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Batch Aggregation using SageMaker PySpark Processing Job]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/17/25 09:30:24] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> <span style=\"font-weight: bold\">[</span>Batch Aggregation using SageMaker PySpark Processing Job<span style=\"font-weight: bold\">]</span>              <a href=\"file:///tmp/ipykernel_7492/321880227.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">321880227.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_7492/321880227.py#1\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/17/25 09:30:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m \u001b[1m[\u001b[0mBatch Aggregation using SageMaker PySpark Processing Job\u001b[1m]\u001b[0m              \u001b]8;id=8668;file:///tmp/ipykernel_7492/321880227.py\u001b\\\u001b[2m321880227.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=862346;file:///tmp/ipykernel_7492/321880227.py#1\u001b\\\u001b[2m1\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info('[Batch Aggregation using SageMaker PySpark Processing Job]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "INPUT_KEY_PREFIX = 'raw_clicks'\n",
    "OUTPUT_KEY_PREFIX = 'aggregated_clicks'\n",
    "LOCAL_DIR = '/home/sagemaker-user/cautious-parakeet/notebooks/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to SageMaker Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/cautious-parakeet/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv to ./clickstream.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://{BUCKET}/{INPUT_KEY_PREFIX}/clickstream.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"s3://{BUCKET}/{INPUT_KEY_PREFIX}/clickstream.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['event_id', 'timestamp', 'customer_id', 'session_id', 'event_type',\n",
      "       'product_id', 'product_category', 'price', 'order_in_session',\n",
      "       'purchased_items', 'total_amount', 'interaction_value',\n",
      "       'cumsum_interactions'],\n",
      "      dtype='object')\n",
      "TOTALLENGTH: 274\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>price</th>\n",
       "      <th>order_in_session</th>\n",
       "      <th>purchased_items</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>interaction_value</th>\n",
       "      <th>cumsum_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9268038f-dbef-4e0c-80e2-a5d7866c6000</td>\n",
       "      <td>2025-03-01 02:01:52</td>\n",
       "      <td>25</td>\n",
       "      <td>0816bcd6-6298-4ab1-8cd8-a2a30bd4de10</td>\n",
       "      <td>page_view</td>\n",
       "      <td>453.0</td>\n",
       "      <td>books</td>\n",
       "      <td>192.33</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bf953c32-9485-46e3-8fba-229a57e6e588</td>\n",
       "      <td>2025-03-01 02:01:56</td>\n",
       "      <td>25</td>\n",
       "      <td>0816bcd6-6298-4ab1-8cd8-a2a30bd4de10</td>\n",
       "      <td>click</td>\n",
       "      <td>453.0</td>\n",
       "      <td>books</td>\n",
       "      <td>38.01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e6bd47c5-1485-45b6-b347-56ddda312547</td>\n",
       "      <td>2025-03-01 02:02:03</td>\n",
       "      <td>25</td>\n",
       "      <td>0816bcd6-6298-4ab1-8cd8-a2a30bd4de10</td>\n",
       "      <td>page_view</td>\n",
       "      <td>453.0</td>\n",
       "      <td>books</td>\n",
       "      <td>457.72</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38dd0f35-4b17-4255-99a8-b1370e7fd726</td>\n",
       "      <td>2025-03-01 02:02:09</td>\n",
       "      <td>25</td>\n",
       "      <td>0816bcd6-6298-4ab1-8cd8-a2a30bd4de10</td>\n",
       "      <td>page_view</td>\n",
       "      <td>396.0</td>\n",
       "      <td>home</td>\n",
       "      <td>175.48</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04f79f4e-0bbc-4c1d-a6e5-276162623e98</td>\n",
       "      <td>2025-03-01 02:02:13</td>\n",
       "      <td>25</td>\n",
       "      <td>0816bcd6-6298-4ab1-8cd8-a2a30bd4de10</td>\n",
       "      <td>click</td>\n",
       "      <td>396.0</td>\n",
       "      <td>home</td>\n",
       "      <td>199.26</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               event_id            timestamp  customer_id  \\\n",
       "0  9268038f-dbef-4e0c-80e2-a5d7866c6000  2025-03-01 02:01:52           25   \n",
       "1  bf953c32-9485-46e3-8fba-229a57e6e588  2025-03-01 02:01:56           25   \n",
       "2  e6bd47c5-1485-45b6-b347-56ddda312547  2025-03-01 02:02:03           25   \n",
       "3  38dd0f35-4b17-4255-99a8-b1370e7fd726  2025-03-01 02:02:09           25   \n",
       "4  04f79f4e-0bbc-4c1d-a6e5-276162623e98  2025-03-01 02:02:13           25   \n",
       "\n",
       "                             session_id event_type  product_id  \\\n",
       "0  0816bcd6-6298-4ab1-8cd8-a2a30bd4de10  page_view       453.0   \n",
       "1  0816bcd6-6298-4ab1-8cd8-a2a30bd4de10      click       453.0   \n",
       "2  0816bcd6-6298-4ab1-8cd8-a2a30bd4de10  page_view       453.0   \n",
       "3  0816bcd6-6298-4ab1-8cd8-a2a30bd4de10  page_view       396.0   \n",
       "4  0816bcd6-6298-4ab1-8cd8-a2a30bd4de10      click       396.0   \n",
       "\n",
       "  product_category   price  order_in_session purchased_items  total_amount  \\\n",
       "0            books  192.33                 1             NaN           NaN   \n",
       "1            books   38.01                 2             NaN           NaN   \n",
       "2            books  457.72                 3             NaN           NaN   \n",
       "3             home  175.48                 4             NaN           NaN   \n",
       "4             home  199.26                 5             NaN           NaN   \n",
       "\n",
       "   interaction_value  cumsum_interactions  \n",
       "0                  4                    4  \n",
       "1                  1                    5  \n",
       "2                  4                    9  \n",
       "3                  4                   13  \n",
       "4                  1                   14  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickstread_df = pd.read_csv(f\"./clickstream.csv\")\n",
    "# set(clickstread_df[\"event_type\"])\n",
    "# Index(['Unnamed: 0', 'event_id', 'timestamp', 'customer_id', 'session_id',\n",
    "#        'event_type', 'product_id', 'product_category', 'price',\n",
    "#        'order_in_session', 'purchased_items', 'total_amount',\n",
    "#        'interaction_value', 'cumsum_interactions'],\n",
    "#       dtype='object')\n",
    "print(clickstread_df.columns)\n",
    "print(f\"TOTALLENGTH: {len(clickstread_df)}\")\n",
    "clickstread_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch_aggregation_user_behavior.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_aggregation_user_behavior.py\n",
    "from pyspark.sql.types import (StructField, StructType, StringType, DoubleType, \n",
    "                               TimestampType, LongType)\n",
    "from pyspark.sql.functions import desc, dense_rank, col, when, count, avg, lower, trim\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Total unique users expected (for validation)\n",
    "TOTAL_UNIQUE_USERS = 10000\n",
    "# Replace with your actual feature group name for batch aggregated user behavior features\n",
    "# aggregate_feature_group_name: cc-agg-fg\n",
    "# aggregate_batch_feature_group_name: cc-agg-batch-fg\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "feature_store_client = boto3.client(service_name='sagemaker-featurestore-runtime')\n",
    "\n",
    "def parse_args() -> Namespace:\n",
    "    parser = ArgumentParser(description='Spark Job Input and Output Args for User Behavior Aggregation')\n",
    "    parser.add_argument('--s3_input_bucket', type=str, help='S3 Input Bucket')\n",
    "    parser.add_argument('--s3_input_key_prefix', type=str, help='S3 Input Key Prefix')\n",
    "    parser.add_argument('--s3_output_bucket', type=str, help='S3 Output Bucket')\n",
    "    parser.add_argument('--s3_output_key_prefix', type=str, help='S3 Output Key Prefix')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def define_schema() -> StructType:\n",
    "    # Define schema based on your sample CSV:\n",
    "    # ['event_id', 'timestamp', 'customer_id', 'session_id', 'event_type',\n",
    "    #    'product_id', 'product_category', 'price', 'order_in_session',\n",
    "    #    'purchased_items', 'total_amount', 'interaction_value',\n",
    "    #    'cumsum_interactions']%%!\n",
    "    \n",
    "    schema = StructType([\n",
    "        # StructField('index', StringType(), True),  # if present as the first column\n",
    "        StructField('event_id', StringType(), True),\n",
    "        StructField('timestamp', TimestampType(), True),\n",
    "        StructField('customer_id', LongType(), True),\n",
    "        StructField('session_id', StringType(), True),\n",
    "        StructField('event_type', StringType(), True),\n",
    "        StructField('product_id', DoubleType(), True),\n",
    "        StructField('product_category', StringType(), True),\n",
    "        StructField('price', DoubleType(), True),\n",
    "        StructField('order_in_session', LongType(), True),\n",
    "        # These fields may be null if not applicable; adjust types as needed:\n",
    "        StructField('purchased_items', LongType(), True),\n",
    "        StructField('total_amount', DoubleType(), True),\n",
    "        StructField('interaction_value', LongType(), True),\n",
    "        StructField('cumsum_interactions', LongType(), True)\n",
    "    ])\n",
    "    return schema\n",
    "\n",
    "def aggregate_features(args: Namespace, schema: StructType, spark: SparkSession) -> DataFrame:\n",
    "    logger.info('[Loggs][Read User Behavior Data as Spark DataFrame]')\n",
    "    input_path = f's3a://{os.path.join(args.s3_input_bucket, args.s3_input_key_prefix)}'\n",
    "    events_df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "\n",
    "    events_df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "    distinct_events = events_df.select('event_type').distinct().collect()\n",
    "    events_df.printSchema()\n",
    "    logger.info(f\"[Loggs] Total rows in input: {events_df.count()} --- {distinct_events}\")\n",
    "    \n",
    "    logger.info('[Loggs][Filter and Aggregate Order Data]')\n",
    "    # We assume that order events are indicated by event_type = 'order'\n",
    "    # If your data marks orders in another way (e.g., non-null total_amount), adjust the filter accordingly.\n",
    "    # orders_df = events_df.filter(col('event_type') == 'purchase')\n",
    "    orders_df = events_df.filter(lower(trim(col('event_type'))) == 'purchase')\n",
    "\n",
    "    logger.info(f\"[Loggs]Order events count: {orders_df.count()}\")\n",
    "    \n",
    "    # Define windows:\n",
    "    # For batch aggregates, we compute aggregates over the past 1 week.\n",
    "    window_1w = Window.partitionBy('customer_id')\\\n",
    "                      .orderBy(col('timestamp').cast(\"long\"))\\\n",
    "                      .rangeBetween(-7 * 24 * 3600, 0)\n",
    "    \n",
    "    aggregated_df = orders_df.withColumn('total_orders_last_1w', count('*').over(window_1w)) \\\n",
    "                              .withColumn('avg_order_value_last_1w', avg(col('total_amount')).over(window_1w))\n",
    "    \n",
    "    # Optionally, you can compute additional features. For example, if you want to include a real-time like metric\n",
    "    # computed over a shorter interval (e.g., last 5 minutes) for clicks:\n",
    "    clicks_df = events_df.filter(col('event_type') == 'click')\n",
    "\n",
    "    clicks_df = events_df.filter(col('event_type') == 'click')\n",
    "    logger.info(f\"[Loggs]Click events count: {clicks_df.count()}\")\n",
    "\n",
    "    window_5m = Window.partitionBy('customer_id')\\\n",
    "                      .orderBy(col('timestamp').cast(\"long\"))\\\n",
    "                      .rangeBetween(-5 * 60, 0)\n",
    "    clicks_agg = clicks_df.withColumn('clicks_last_5m', count('*').over(window_5m))\n",
    "    \n",
    "    # Join the aggregates by customer_id. Depending on your needs, you may join batch and streaming features,\n",
    "    # or process them in separate feature groups.\n",
    "    agg_joined = aggregated_df.join(clicks_agg.select('customer_id', 'clicks_last_5m'),\n",
    "                                    on='customer_id', how='left')\n",
    "    \n",
    "    # Remove duplicates by taking the latest record per customer.\n",
    "    window_latest = Window.partitionBy('customer_id').orderBy(desc('timestamp'))\n",
    "    sorted_df = agg_joined.withColumn('rank', dense_rank().over(window_latest))\n",
    "    grouped_df = sorted_df.filter(col('rank') == 1).drop('rank')\n",
    "    logger.info(f\"[Loggs]aggregate_features - grouped_df count: {grouped_df.count()}\")\n",
    "    # Select the fields that match your feature group schema\n",
    "    # Here, we assume the batch feature group expects: user_id, total_orders_last_1w, avg_order_value_last_1w, clicks_last_5m, event_time\n",
    "    final_df = grouped_df.select(\n",
    "        col('customer_id').alias('user_id'),\n",
    "        col('total_orders_last_1w'),\n",
    "        col('avg_order_value_last_1w'),\n",
    "        col('clicks_last_5m'),\n",
    "        col('timestamp').alias('event_time')\n",
    "    )\n",
    "    return final_df\n",
    "\n",
    "def write_to_s3(args: Namespace, aggregated_features: DataFrame) -> None:\n",
    "    logger.info(f'[Loggs][Write Aggregated Features to S3] {aggregated_features.count()}')\n",
    "    output_path = 's3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix)\n",
    "    aggregated_features.coalesce(1) \\\n",
    "                       .write.format('csv') \\\n",
    "                       .option('header', True) \\\n",
    "                       .mode('overwrite') \\\n",
    "                       .save(output_path)\n",
    "    \n",
    "def group_by_user(aggregated_features: DataFrame) -> DataFrame: \n",
    "    logger.info('[Loggs][Group Aggregated Features by User]')\n",
    "    window = Window.partitionBy('user_id').orderBy(desc('event_time'))\n",
    "    sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "    logger.info(f'[Loggs]sorted_df count {sorted_df.count()}')\n",
    "    grouped_df = sorted_df.filter(col('rank') == 1).drop('rank')\n",
    "    logger.info(f'[Loggs]grouped_df count {grouped_df.count()}')\n",
    "    # Select only the columns to be ingested into the feature store\n",
    "    sliced_df = grouped_df.select('user_id', 'total_orders_last_1w', 'avg_order_value_last_1w', 'clicks_last_5m')\n",
    "    logger.info(f'[Loggs]sliced_df count {sliced_df.count()}')\n",
    "    return sliced_df\n",
    "\n",
    "def transform_row(sliced_df: DataFrame) -> list:\n",
    "    logger.info('[Loggs][Transform Spark DataFrame Rows to Feature Store Records]')\n",
    "    records = []\n",
    "    for row in sliced_df.rdd.collect():\n",
    "        record = []\n",
    "        user_id, total_orders_last_1w, avg_order_value_last_1w, clicks_last_5m = row\n",
    "        if user_id is not None:\n",
    "            record.append({'FeatureName': 'user_id', 'ValueAsString': str(user_id)})\n",
    "            record.append({'FeatureName': 'total_orders_last_1w', 'ValueAsString': str(total_orders_last_1w)})\n",
    "            record.append({'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': str(round(avg_order_value_last_1w, 2) if avg_order_value_last_1w else 0.0)})\n",
    "            # record.append({'FeatureName': 'clicks_last_5m', 'ValueAsString': str(clicks_last_5m if clicks_last_5m else 0)})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def write_to_feature_store(records: list) -> None:\n",
    "    logger.info(f'[Loggs][Write Grouped Features to SageMaker Feature Store] records count- {len(records)}')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'event_time',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "        logger.info('[Loggs]Success = {}, Failed = {}'.format(success, fail))\n",
    "    logger.info('[Loggs]Success = {}'.format(success))\n",
    "    logger.info('[Loggs]Fail = {}'.format(fail))\n",
    "    # You can adjust these assertions based on your expected unique user count\n",
    "    assert success <= TOTAL_UNIQUE_USERS\n",
    "    assert fail == 0\n",
    "\n",
    "def run_spark_job():\n",
    "    spark = SparkSession.builder.appName('UserBehaviorBatchAggregationJob5').getOrCreate()\n",
    "    args = parse_args()\n",
    "    schema = define_schema()\n",
    "    aggregated_features = aggregate_features(args, schema, spark)\n",
    "    write_to_s3(args, aggregated_features)\n",
    "    grouped_features = group_by_user(aggregated_features)\n",
    "    records = transform_row(grouped_features)\n",
    "    write_to_feature_store(records)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run_spark_job()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Read in the file\n",
    "# with open('batch_aggregation_temp.py', 'r') as file :\n",
    "#   filedata = file.read()\n",
    "\n",
    "# # Replace the target string\n",
    "# filedata = filedata.replace('|agg_batch_fg_name|', aggregate_batch_feature_group_name)\n",
    "\n",
    "# # Write the file out again\n",
    "# with open('batch_aggregation.py', 'w') as file:\n",
    "#   file.write(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(base_job_name='sagemaker-processing', \n",
    "                                   framework_version='2.4', # spark version\n",
    "                                   role=sagemaker_role, \n",
    "                                   # instance_count=1, \n",
    "                                   # instance_type='ml.m5.4xlarge', \n",
    "                                   instance_count=2, \n",
    "                                   instance_type='ml.t3.xlarge', \n",
    "                                   env={'AWS_DEFAULT_REGION': boto3.Session().region_name},\n",
    "                                   max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sagemaker-processing-2025-03-17-09-30-46-359\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/17/25 09:30:46] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating processing-job with name                                      <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1575</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         sagemaker-processing-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-17-09-30-46-359                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/17/25 09:30:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating processing-job with name                                      \u001b]8;id=327331;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=985342;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1575\u001b\\\u001b[2m1575\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         sagemaker-processing-\u001b[1;36m2025\u001b[0m-03-17-09-30-46-359                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................\u001b[35m03-17 09:33 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/batch_aggregation_user_behavior.py', '--s3_input_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_input_key_prefix', 'raw_clicks', '--s3_output_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_output_key_prefix', 'aggregated_clicks']\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/batch_aggregation_user_behavior.py', '--s3_input_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_input_key_prefix', 'raw_clicks', '--s3_output_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_output_key_prefix', 'aggregated_clicks']\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     {'current_host': 'algo-2', 'current_instance_type': 'ml.t3.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.t3.xlarge', 'hosts': ['algo-2', 'algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:ap-southeast-1:850995562355:processing-job/sagemaker-processing-2025-03-17-09-30-46-359', 'ProcessingJobName': 'sagemaker-processing-2025-03-17-09-30-46-359', 'Environment': {'AWS_DEFAULT_REGION': 'ap-southeast-1'}, 'AppSpecification': {'ImageUri': '759080221371.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/batch_aggregation_user_behavior.py'], 'ContainerArguments': ['--s3_input_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_input_key_prefix', 'raw_clicks', '--s3_output_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_output_key_prefix', 'aggregated_clicks']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-ap-southeast-1-850995562355/sagemaker-processing-2025-03-17-09-30-46-359/input/code/batch_aggregation_user_behavior.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://sagemaker-ap-southeast-1-850995562355/logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.t3.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::850995562355:role/sagemaker-featurestore-msk-k-SageMakerExecutionRole-DGG4W0Tc3E0W', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/batch_aggregation_user_behavior.py --s3_input_bucket sagemaker-ap-southeast-1-850995562355 --s3_input_key_prefix raw_clicks --s3_output_bucket sagemaker-ap-southeast-1-850995562355 --s3_output_key_prefix aggregated_clicks\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     Status server listening on algo-2:5555\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[35mServing on http://algo-2:5555\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     Found hadoop jar hadoop-aws.jar\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[35m03-17 09:33 root         INFO     Detected instance type: t3.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[35m03-17 09:33 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m03-17 09:33 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[35m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[35m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.130.83</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-2</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-2:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[35m</configuration>\u001b[0m\n",
      "\u001b[35m03-17 09:33 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m03-17 09:33 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[35mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.driver.host=10.0.130.83\u001b[0m\n",
      "\u001b[35mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[35mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[35mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[35mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[35mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[35mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[35mspark.executor.cores 4\u001b[0m\n",
      "\u001b[35mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[35mspark.executor.instances 2\u001b[0m\n",
      "\u001b[35mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[35m03-17 09:33 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[35m03-17 09:33 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:50 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.164.134\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapred\u001b[0m\n",
      "\u001b[35muce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:50 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:50 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.164.134\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/\u001b[0m\n",
      "\u001b[35m.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:50 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4a83a74a\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:51 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@710c2b53\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.4 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO datanode.DataNode: Configured hostname is algo-2\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Jetty bound to port 33301\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:38631\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.164.134:38631\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/10.0.164.134:0\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:52 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33301\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.130.83:8020 starting to offer service\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:53 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.2_8042_node____.8nf84m/webapp\u001b[0m\n",
      "\u001b[35mMar 17, 2025 9:33:53 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[35mMar 17, 2025 9:33:53 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[35mMar 17, 2025 9:33:53 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[35mMar 17, 2025 9:33:53 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[35mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[35mMar 17, 2025 9:33:53 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mMar 17, 2025 9:33:54 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mMar 17, 2025 9:33:54 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-2:8042\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:38631\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO client.RMProxy: Connecting to ResourceManager at /10.0.130.83:8031\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     waiting for the primary to come up\u001b[0m\n",
      "\u001b[35mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[35m03-17 09:33 smspark-submit INFO     waiting for the primary to go down\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:54 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:55 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:55 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:56 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:56 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:57 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:57 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/batch_aggregation_user_behavior.py', '--s3_input_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_input_key_prefix', 'raw_clicks', '--s3_output_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_output_key_prefix', 'aggregated_clicks']\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/batch_aggregation_user_behavior.py', '--s3_input_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_input_key_prefix', 'raw_clicks', '--s3_output_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_output_key_prefix', 'aggregated_clicks']\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     {'current_host': 'algo-1', 'current_instance_type': 'ml.t3.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.t3.xlarge', 'hosts': ['algo-2', 'algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:ap-southeast-1:850995562355:processing-job/sagemaker-processing-2025-03-17-09-30-46-359', 'ProcessingJobName': 'sagemaker-processing-2025-03-17-09-30-46-359', 'Environment': {'AWS_DEFAULT_REGION': 'ap-southeast-1'}, 'AppSpecification': {'ImageUri': '759080221371.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/batch_aggregation_user_behavior.py'], 'ContainerArguments': ['--s3_input_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_input_key_prefix', 'raw_clicks', '--s3_output_bucket', 'sagemaker-ap-southeast-1-850995562355', '--s3_output_key_prefix', 'aggregated_clicks']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-ap-southeast-1-850995562355/sagemaker-processing-2025-03-17-09-30-46-359/input/code/batch_aggregation_user_behavior.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://sagemaker-ap-southeast-1-850995562355/logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.t3.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::850995562355:role/sagemaker-featurestore-msk-k-SageMakerExecutionRole-DGG4W0Tc3E0W', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/batch_aggregation_user_behavior.py --s3_input_bucket sagemaker-ap-southeast-1-850995562355 --s3_input_key_prefix raw_clicks --s3_output_bucket sagemaker-ap-southeast-1-850995562355 --s3_output_key_prefix aggregated_clicks\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     Found hadoop jar hadoop-aws.jar\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m03-17 09:33 root         INFO     Detected instance type: t3.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[34m03-17 09:33 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m03-17 09:33 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.130.83</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\u001b[0m\n",
      "\u001b[34m03-17 09:33 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m03-17 09:33 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.130.83\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 4\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 2\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[34m03-17 09:33 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m03-17 09:33 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:55 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.130.83\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.\u001b[0m\n",
      "\u001b[34m0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:55 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:55 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-d2ba026e-d764-4b9f-abf7-18f42a3ac74f\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Mar 17 09:33:56\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSImage: Allocated new BlockPoolId: BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 322 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:56 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.130.83\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m03-17 09:33 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.130.83\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-am\u001b[0m\n",
      "\u001b[34mzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop/etc/hadoop/rm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.130.83\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-am\u001b[0m\n",
      "\u001b[34mzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.130.83\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.\u001b[0m\n",
      "\u001b[34m0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.130.83\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.\u001b[0m\n",
      "\u001b[34m0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:58 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:58 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:59 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:33:59 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:58 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO namenode.NameNode: fs.defaultFS is hdfs://10.0.130.83/\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO security.Groups: clearing userToGroupsMap cache\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:33:59 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:00 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:00 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO http.HttpServer2: Jetty bound to port 50070\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4a83a74a\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@710c2b53\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:00 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.4 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:01 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:01 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.WorkflowPriorityMappingsManager: Initialized workflow priority mappings, override: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:01 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:02 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:02 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO http.HttpServer2: Jetty bound to port 38997\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:43179\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.130.83:43179\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.130.83:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Mar 17 09:34:02\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38997\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 98@algo-1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:02 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/cluster to work/Jetty_10_0_130_83_8088_cluster____.7g5tbt/webapp\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSImageFormatPBINode: Successfully loaded 1 inodes\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:03 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:03 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:03 WARN datanode.DataNode: Problem connecting to server: algo-1/10.0.130.83:8020\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO namenode.FSNamesystem: Finished loading FSImage in 641 msecs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.130.83:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:03 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.NameNode: Enable NameNode state context:false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.1_8042_node____.afclh/webapp\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 15 msec\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:04 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8031. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.130.83:8020\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO namenode.FSDirectory: Quota initialization completed in 27 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:04 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.130.83:8020\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:04 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 99@algo-1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 26696793. Formatting...\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Generated new storageID DS-7348eee7-ac69-463b-93f7-faa0e6371cf7 for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Analyzing storage directories for bpid BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289 is not formatted for BP-128958768-10.0.130.83-1742204036289. Formatting ...\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO common.Storage: Formatting block pool BP-128958768-10.0.130.83-1742204036289 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO datanode.DataNode: Setting up storage: nsid=26696793;bpid=BP-128958768-10.0.130.83-1742204036289;lv=-57;nsInfo=lv=-63;cid=CID-d2ba026e-d764-4b9f-abf7-18f42a3ac74f;nsid=26696793;c=1742204036289;bpid=BP-128958768-10.0.130.83-1742204036289;dnuuid=null\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO datanode.DataNode: Generated and persisted new Datanode UUID a1b6f404-e2f3-46f4-8b21-d2747151bdf5\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Added new volume: DS-7348eee7-ac69-463b-93f7-faa0e6371cf7\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Adding block pool BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Scanning block pool BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:05 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-128958768-10.0.130.83-1742204036289 on /opt/amazon/hadoop/hdfs/datanode/current: 182ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-128958768-10.0.130.83-1742204036289: 200ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode/current: 52ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-128958768-10.0.130.83-1742204036289: 58ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO datanode.VolumeScanner: Now scanning bpid BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-7348eee7-ac69-463b-93f7-faa0e6371cf7): finished scanning block pool BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/17/25 10:44 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:05 INFO datanode.DataNode: Block pool BP-128958768-10.0.130.83-1742204036289 (Datanode Uuid a1b6f404-e2f3-46f4-8b21-d2747151bdf5) service to algo-1/10.0.130.83:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-7348eee7-ac69-463b-93f7-faa0e6371cf7): no suitable block pools found to scan.  Waiting 1814399894 ms.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.130.83:50010, datanodeUuid=a1b6f404-e2f3-46f4-8b21-d2747151bdf5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d2ba026e-d764-4b9f-abf7-18f42a3ac74f;nsid=26696793;c=1742204036289) storage a1b6f404-e2f3-46f4-8b21-d2747151bdf5\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.130.83:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO blockmanagement.BlockReportLeaseManager: Registered DN a1b6f404-e2f3-46f4-8b21-d2747151bdf5 (10.0.130.83:50010).\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO datanode.DataNode: Block pool Block pool BP-128958768-10.0.130.83-1742204036289 (Datanode Uuid a1b6f404-e2f3-46f4-8b21-d2747151bdf5) service to algo-1/10.0.130.83:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO datanode.DataNode: For namenode algo-1/10.0.130.83:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:06 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7348eee7-ac69-463b-93f7-faa0e6371cf7 for DN 10.0.130.83:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@10.0.130.83:8088\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO BlockStateChange: BLOCK* processReport 0x62b22e5068c7fb7: Processing first storage report for DS-7348eee7-ac69-463b-93f7-faa0e6371cf7 from datanode a1b6f404-e2f3-46f4-8b21-d2747151bdf5\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO BlockStateChange: BLOCK* processReport 0x62b22e5068c7fb7: from storage DS-7348eee7-ac69-463b-93f7-faa0e6371cf7 node DatanodeRegistration(10.0.130.83:50010, datanodeUuid=a1b6f404-e2f3-46f4-8b21-d2747151bdf5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d2ba026e-d764-4b9f-abf7-18f42a3ac74f;nsid=26696793;c=1742204036289), blocks: 0, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO datanode.DataNode: Successfully sent block report 0x62b22e5068c7fb7,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 21 msec to generate and 153 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO datanode.DataNode: Got finalize command for block pool BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:06 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34mMar 17, 2025 9:34:06 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-1:8042\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:43179\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO client.RMProxy: Connecting to ResourceManager at /10.0.130.83:8031\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m03-17 09:34 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m03-17 09:34 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m03-17 09:34 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m03-17 09:34 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m03-17 09:34 sagemaker-spark-event-logs-publisher INFO     Start to copy the spark event logs file.\u001b[0m\n",
      "\u001b[34m03-17 09:34 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m03-17 09:34 sagemaker-spark-event-logs-publisher INFO     Writing event log config to spark-defaults.conf\u001b[0m\n",
      "\u001b[34m03-17 09:34 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m03-17 09:34 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2025-03-17T09:34:07.653502')), ('algo-2', StatusMessage(status='WAITING', timestamp='2025-03-17T09:34:07.660465'))])\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:07 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 43179 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:43179\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO rmnode.RMNodeImpl: algo-1:43179 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO capacity.CapacityScheduler: Added node algo-1:43179 clusterResource: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1259797828\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -75927933\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:43179 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.130.83:8020\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 19@algo-2\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 26696793. Formatting...\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Generated new storageID DS-a5276511-8628-4dae-adac-2fa84fcb2526 for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Analyzing storage directories for bpid BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289 is not formatted for BP-128958768-10.0.130.83-1742204036289. Formatting ...\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO common.Storage: Formatting block pool BP-128958768-10.0.130.83-1742204036289 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: Setting up storage: nsid=26696793;bpid=BP-128958768-10.0.130.83-1742204036289;lv=-57;nsInfo=lv=-63;cid=CID-d2ba026e-d764-4b9f-abf7-18f42a3ac74f;nsid=26696793;c=1742204036289;bpid=BP-128958768-10.0.130.83-1742204036289;dnuuid=null\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: Generated and persisted new Datanode UUID b0bcba68-60ed-4a1a-8061-5b622e4ffc11\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Added new volume: DS-a5276511-8628-4dae-adac-2fa84fcb2526\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Adding block pool BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Scanning block pool BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-128958768-10.0.130.83-1742204036289 on /opt/amazon/hadoop/hdfs/datanode/current: 32ms\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-128958768-10.0.130.83-1742204036289: 34ms\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode/current: 1ms\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-128958768-10.0.130.83-1742204036289: 3ms\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.VolumeScanner: Now scanning bpid BP-128958768-10.0.130.83-1742204036289 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-a5276511-8628-4dae-adac-2fa84fcb2526): finished scanning block pool BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/17/25 11:35 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: Block pool BP-128958768-10.0.130.83-1742204036289 (Datanode Uuid b0bcba68-60ed-4a1a-8061-5b622e4ffc11) service to algo-1/10.0.130.83:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-a5276511-8628-4dae-adac-2fa84fcb2526): no suitable block pools found to scan.  Waiting 1814399952 ms.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.164.134:50010, datanodeUuid=b0bcba68-60ed-4a1a-8061-5b622e4ffc11, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d2ba026e-d764-4b9f-abf7-18f42a3ac74f;nsid=26696793;c=1742204036289) storage b0bcba68-60ed-4a1a-8061-5b622e4ffc11\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.164.134:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO blockmanagement.BlockReportLeaseManager: Registered DN b0bcba68-60ed-4a1a-8061-5b622e4ffc11 (10.0.164.134:50010).\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-a5276511-8628-4dae-adac-2fa84fcb2526 for DN 10.0.164.134:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO BlockStateChange: BLOCK* processReport 0xba189b7613a123d1: Processing first storage report for DS-a5276511-8628-4dae-adac-2fa84fcb2526 from datanode b0bcba68-60ed-4a1a-8061-5b622e4ffc11\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:08 INFO BlockStateChange: BLOCK* processReport 0xba189b7613a123d1: from storage DS-a5276511-8628-4dae-adac-2fa84fcb2526 node DatanodeRegistration(10.0.164.134:50010, datanodeUuid=b0bcba68-60ed-4a1a-8061-5b622e4ffc11, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d2ba026e-d764-4b9f-abf7-18f42a3ac74f;nsid=26696793;c=1742204036289), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: Block pool Block pool BP-128958768-10.0.130.83-1742204036289 (Datanode Uuid b0bcba68-60ed-4a1a-8061-5b622e4ffc11) service to algo-1/10.0.130.83:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: For namenode algo-1/10.0.130.83:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: Successfully sent block report 0xba189b7613a123d1,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 22 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:08 INFO datanode.DataNode: Got finalize command for block pool BP-128958768-10.0.130.83-1742204036289\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:12 INFO spark.SparkContext: Running Spark version 2.4.6-amzn-0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:12 INFO spark.SparkContext: Submitted application: UserBehaviorBatchAggregationJob5\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:12 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:12 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:12 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:12 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:12 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:13 INFO util.Utils: Successfully started service 'sparkDriver' on port 46483.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:13 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-98e5fef9-e8a9-4412-960e-7c069e0ac41b\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO memory.MemoryStore: MemoryStore started with capacity 1028.8 MB\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO util.log: Logging initialized @6543ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO server.Server: Started @6733ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO server.AbstractConnector: Started ServerConnector@1f412009{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7710149{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6dd8d14{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e7e2825{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30c10962{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a9e8743{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@255eb335{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50311722{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@169e7c6c{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ecef19a{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4bef6a{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@310d1510{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54957241{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d6e0b68{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@83b8910{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51c7b892{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ebfcf91{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3397b465{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6730c80e{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b5fff52{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b4c6aef{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55f6c97f{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@121db4ac{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@349f709e{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2efdae82{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2107704f{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:14 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.130.83:4040\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO client.RMProxy: Connecting to ResourceManager at /10.0.130.83:8032\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:17 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:18 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:26 INFO yarn.Client: Uploading resource file:/tmp/spark-7606e37b-d1be-455f-8687-46fa277f10a6/__spark_libs__478442699550967940.zip -> hdfs://10.0.130.83/user/root/.sparkStaging/application_1742204047158_0001/__spark_libs__478442699550967940.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:26 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.0.130.83:50010, 10.0.164.134:50010 for /user/root/.sparkStaging/application_1742204047158_0001/__spark_libs__478442699550967940.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:26 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741825_1001 src: /10.0.130.83:48092 dest: /10.0.130.83:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:27 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741825_1001 src: /10.0.130.83:54492 dest: /10.0.164.134:50010\u001b[0m\n",
      "\u001b[34m03-17 09:34 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:28 INFO DataNode.clienttrace: src: /10.0.130.83:54492, dest: /10.0.164.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: b0bcba68-60ed-4a1a-8061-5b622e4ffc11, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741825_1001, duration(ns): 1399168449\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:28 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:28 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741826_1002 src: /10.0.130.83:54498 dest: /10.0.164.134:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:28 INFO DataNode.clienttrace: src: /10.0.130.83:48092, dest: /10.0.130.83:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: a1b6f404-e2f3-46f4-8b21-d2747151bdf5, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741825_1001, duration(ns): 1399918787\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:28 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.164.134:50010] terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:28 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.0.130.83:50010, 10.0.164.134:50010 for /user/root/.sparkStaging/application_1742204047158_0001/__spark_libs__478442699550967940.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:28 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741826_1002 src: /10.0.130.83:48094 dest: /10.0.130.83:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:29 INFO DataNode.clienttrace: src: /10.0.130.83:54498, dest: /10.0.164.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: b0bcba68-60ed-4a1a-8061-5b622e4ffc11, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741826_1002, duration(ns): 890434790\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:29 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:29 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741827_1003 src: /10.0.130.83:54510 dest: /10.0.164.134:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:29 INFO DataNode.clienttrace: src: /10.0.130.83:48094, dest: /10.0.130.83:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: a1b6f404-e2f3-46f4-8b21-d2747151bdf5, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741826_1002, duration(ns): 891712253\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:29 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.164.134:50010] terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:29 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.0.130.83:50010, 10.0.164.134:50010 for /user/root/.sparkStaging/application_1742204047158_0001/__spark_libs__478442699550967940.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:29 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741827_1003 src: /10.0.130.83:48098 dest: /10.0.130.83:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:54510, dest: /10.0.164.134:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: b0bcba68-60ed-4a1a-8061-5b622e4ffc11, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741827_1003, duration(ns): 708482413\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741828_1004 src: /10.0.130.83:54516 dest: /10.0.164.134:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:54516, dest: /10.0.164.134:50010, bytes: 13532432, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: b0bcba68-60ed-4a1a-8061-5b622e4ffc11, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741828_1004, duration(ns): 95366686\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741829_1005 src: /10.0.130.83:54528 dest: /10.0.164.134:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:54528, dest: /10.0.164.134:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: b0bcba68-60ed-4a1a-8061-5b622e4ffc11, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741829_1005, duration(ns): 11043386\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741830_1006 src: /10.0.130.83:54532 dest: /10.0.164.134:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:54532, dest: /10.0.164.134:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: b0bcba68-60ed-4a1a-8061-5b622e4ffc11, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741830_1006, duration(ns): 26231787\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:48098, dest: /10.0.130.83:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: a1b6f404-e2f3-46f4-8b21-d2747151bdf5, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741827_1003, duration(ns): 711735453\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.164.134:50010] terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.0.130.83:50010, 10.0.164.134:50010 for /user/root/.sparkStaging/application_1742204047158_0001/__spark_libs__478442699550967940.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741828_1004 src: /10.0.130.83:48102 dest: /10.0.130.83:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:48102, dest: /10.0.130.83:50010, bytes: 13532432, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: a1b6f404-e2f3-46f4-8b21-d2747151bdf5, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741828_1004, duration(ns): 88918616\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.164.134:50010] terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1742204047158_0001/__spark_libs__478442699550967940.zip is closed by DFSClient_NONMAPREDUCE_1651841552_18\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.0.130.83/user/root/.sparkStaging/application_1742204047158_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.0.130.83:50010, 10.0.164.134:50010 for /user/root/.sparkStaging/application_1742204047158_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741829_1005 src: /10.0.130.83:48104 dest: /10.0.130.83:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:48104, dest: /10.0.130.83:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: a1b6f404-e2f3-46f4-8b21-d2747151bdf5, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741829_1005, duration(ns): 17287982\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.164.134:50010] terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1742204047158_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_1651841552_18\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.130.83/user/root/.sparkStaging/application_1742204047158_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.0.130.83:50010, 10.0.164.134:50010 for /user/root/.sparkStaging/application_1742204047158_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741830_1006 src: /10.0.130.83:48106 dest: /10.0.130.83:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO DataNode.clienttrace: src: /10.0.130.83:48106, dest: /10.0.130.83:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: a1b6f404-e2f3-46f4-8b21-d2747151bdf5, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741830_1006, duration(ns): 4963518\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.164.134:50010] terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:30 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1742204047158_0001/py4j-0.10.7-src.zip is closed by DFSClient_NONMAPREDUCE_1651841552_18\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:31 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741831_1007 src: /10.0.130.83:54542 dest: /10.0.164.134:50010\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:31 INFO DataNode.clienttrace: src: /10.0.130.83:54542, dest: /10.0.164.134:50010, bytes: 245234, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: b0bcba68-60ed-4a1a-8061-5b622e4ffc11, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741831_1007, duration(ns): 16069504\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:31 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO yarn.Client: Uploading resource file:/tmp/spark-7606e37b-d1be-455f-8687-46fa277f10a6/__spark_conf__542572090323217154.zip -> hdfs://10.0.130.83/user/root/.sparkStaging/application_1742204047158_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.0.130.83:50010, 10.0.164.134:50010 for /user/root/.sparkStaging/application_1742204047158_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO datanode.DataNode: Receiving BP-128958768-10.0.130.83-1742204036289:blk_1073741831_1007 src: /10.0.130.83:48116 dest: /10.0.130.83:50010\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO DataNode.clienttrace: src: /10.0.130.83:48116, dest: /10.0.130.83:50010, bytes: 245234, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651841552_18, offset: 0, srvID: a1b6f404-e2f3-46f4-8b21-d2747151bdf5, blockid: BP-128958768-10.0.130.83-1742204036289:blk_1073741831_1007, duration(ns): 16782857\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1742204047158_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1651841552_18\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO datanode.DataNode: PacketResponder: BP-128958768-10.0.130.83-1742204036289:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.164.134:50010] terminating\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO yarn.Client: Submitting application application_1742204047158_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 38631 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:38631\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO rmnode.RMNodeImpl: algo-2:38631 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.CapacityScheduler: Added node algo-2:38631 clusterResource: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.CapacityScheduler: Application 'application_1742204047158_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO rmapp.RMAppImpl: Storing application with id application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO recovery.RMStateStore: Storing info for app: application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.130.83#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO rmapp.RMAppImpl: application_1742204047158_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO rmapp.RMAppImpl: application_1742204047158_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.ParentQueue: Application added - appId: application_1742204047158_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.CapacityScheduler: Accepted application application_1742204047158_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO rmapp.RMAppImpl: application_1742204047158_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.LeafQueue: Application application_1742204047158_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.LeafQueue: Application added - appId: application_1742204047158_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1742204047158_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO impl.YarnClientImpl: Submitted application application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:34 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1742204047158_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1742204047158_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:38631 for container : container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1742204047158_0001 AttemptId: appattempt_1742204047158_0001_000001 MasterContainer: Container: [ContainerId: container_1742204047158_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:38631, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.164.134:38631 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO amlauncher.AMLauncher: Launching masterappattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1742204047158_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:38631, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.164.134:38631 }, ExecutionType: GUARANTEED, ] for AM appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:35 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Mar 17 09:34:35 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1742204074626\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1742204047158_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:34 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1259797828\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:34 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -75927933\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:34 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:38631 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO ipc.Server: Auth successful for appattempt_1742204047158_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO containermanager.ContainerManagerImpl: Start request for container_1742204047158_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.130.83#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO application.ApplicationImpl: Adding container_1742204047158_0001_01_000001 to application application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO localizer.ResourceLocalizationService: Created localizer for container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1742204047158_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1742204047158_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:36 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:36 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1742204047158_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:38631, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.164.134:38631 }, ExecutionType: GUARANTEED, ] for AM appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:36 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:36 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1742204047158_0001, attemptId: appattempt_1742204047158_0001_000001launchTime: 1742204076310\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:36 INFO recovery.RMStateStore: Updating info for app: application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:36 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:36 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:37 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:38 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:39 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:39 INFO scheduler.ContainerScheduler: Starting container [container_1742204047158_0001_01_000001]\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:39 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:39 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:39 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:39 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:40 INFO monitor.ContainersMonitorImpl: container_1742204047158_0001_01_000001's ip = 10.0.164.134, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:40 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1742204047158_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:40 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:41 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:42 INFO yarn.Client: Application report for application_1742204047158_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO ipc.Server: Auth successful for appattempt_1742204047158_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.164.134#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011APPATTEMPTID=appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO rmapp.RMAppImpl: application_1742204047158_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1742204047158_0001), /proxy/application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO yarn.Client: Application report for application_1742204047158_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.164.134\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1742204074626\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1742204047158_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO cluster.YarnClientSchedulerBackend: Application application_1742204047158_0001 has started running.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42973.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO netty.NettyBlockTransferService: Server created on 10.0.130.83:42973\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:43 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.130.83, 42973, None)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.130.83:42973 with 1028.8 MB RAM, BlockManagerId(driver, 10.0.130.83, 42973, None)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.130.83, 42973, None)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.130.83, 42973, None)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e8092ef{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1742204047158_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4572741 absoluteUsedCapacity=0.4572741 used=<memory:14534, vCores:2> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1742204047158_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4572741 absoluteUsedCapacity=0.4572741 used=<memory:14534, vCores:2> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000003 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8863579 absoluteUsedCapacity=0.8863579 used=<memory:28172, vCores:3> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:38631 for container : container_1742204047158_0001_01_000002\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:43179 for container : container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:44 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO scheduler.EventLoggingListener: Logging events to file:/tmp/spark-events/application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO ipc.Server: Auth successful for appattempt_1742204047158_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO containermanager.ContainerManagerImpl: Start request for container_1742204047158_0001_01_000003 by user root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO application.ApplicationImpl: Adding container_1742204047158_0001_01_000003 to application application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.164.134#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000003 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO localizer.ResourceLocalizationService: Created localizer for container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15f467e8{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64017970{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21956962{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32034c69{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ed7512e{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1742204047158_0001_01_000003.tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1742204047158_0001_01_000003.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000003.tokens\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:45 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:40 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:41 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:42 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:42 INFO client.RMProxy: Connecting to ResourceManager at /10.0.130.83:8030\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:43 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:43 INFO client.TransportClientFactory: Successfully created connection to /10.0.130.83:46483 after 153 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:43 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] YARN executor launch context:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.0.130.83/user/root/.sparkStaging/application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     SPARK_NO_DAEMONIZE -> TRUE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     SPARK_MASTER_HOST -> 10.0.130.83\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     SPARK_HOME -> /usr/lib/spark\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       -Xmx12399m \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       '-Dspark.driver.port=46483' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       org.apache.spark.executor.CoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.130.83:46483 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       4 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       application_1742204047158_0001 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.0.130.83\" port: -1 file: \"/user/root/.sparkStaging/application_1742204047158_0001/pyspark.zip\" } size: 596339 timestamp: 1742204070755 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"10.0.130.83\" port: -1 file: \"/user/root/.sparkStaging/application_1742204047158_0001/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1742204070838 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.0.130.83\" port: -1 file: \"/user/root/.sparkStaging/application_1742204047158_0001/__spark_libs__478442699550967940.zip\" } size: 416185616 timestamp: 1742204070364 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.0.130.83\" port: -1 file: \"/user/root/.sparkStaging/application_1742204047158_0001/__spark_conf__.zip\" } size: 245234 timestamp: 1742204071417 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:43 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:43 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:43 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000001/stderr] 25/03/17 09:34:43 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO ipc.Server: Auth successful for appattempt_1742204047158_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO containermanager.ContainerManagerImpl: Start request for container_1742204047158_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.164.134#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000002\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO application.ApplicationImpl: Adding container_1742204047158_0001_01_000002 to application application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO scheduler.ContainerScheduler: Starting container [container_1742204047158_0001_01_000002]\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1742204047158_0001_01_000002\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:44 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_Handling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:46 INFO monitor.ContainersMonitorImpl: container_1742204047158_0001_01_000002's ip = 10.0.164.134, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m25/03/17 09:34:46 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1742204047158_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:47 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m[Loggs][Read User Behavior Data as Spark DataFrame]\u001b[0m\n",
      "\u001b[34m03-17 09:34 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1742204047158_0001.inprogress\u001b[0m\n",
      "\u001b[34m03-17 09:34 root         INFO     copying /tmp/spark-events/application_1742204047158_0001.inprogress to /opt/ml/processing/spark-events/application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:47 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:48 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.164.134:53210) with ID 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:37179 with 6.3 GB RAM, BlockManagerId(1, algo-2, 37179, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 459@algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:46 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:47 INFO client.TransportClientFactory: Successfully created connection to /10.0.130.83:46483 after 120 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO client.TransportClientFactory: Successfully created connection to /10.0.130.83:46483 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/blockmgr-281e8a98-1ca7-4ba6-8d39-7df474dcb813\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GB\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:51 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:52 INFO datasources.InMemoryFileIndex: It took 288 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:52 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000003 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:52 INFO scheduler.ContainerScheduler: Starting container [container_1742204047158_0001_01_000003]\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:52 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000003 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:52 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:52 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000003/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:53 INFO monitor.ContainersMonitorImpl: container_1742204047158_0001_01_000003's ip = 10.0.130.83, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:53 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1742204047158_0001_01_000003 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:54 INFO datasources.InMemoryFileIndex: It took 99 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:55 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:55 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:55 INFO datasources.FileSourceStrategy: Output Data Schema: struct<event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:55 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:55 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:34:56 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:56 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:57 INFO codegen.CodeGenerator: Code generated in 899.140849 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:57 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KB, free 1028.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:58 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:58 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:58 INFO spark.SparkContext: Created broadcast 0 from collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:58 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:58 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.DAGScheduler: Registering RDD 2 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.DAGScheduler: Got map stage job 0 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 20.3 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.1 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.130.83:42973 (size: 10.1 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:34:59 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:37179 (size: 10.1 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:01 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.130.83:58014) with ID 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:01 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.130.83:46483\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO executor.Executor: Starting executor ID 1 on host algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37179.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO netty.NettyBlockTransferService: Server created on algo-2:37179\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:48 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:49 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-2, 37179, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:49 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-2, 37179, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:49 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-2, 37179, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:51 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:51 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 2917 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO client.TransportClientFactory: Successfully created connection to /10.0.130.83:42973 after 2 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:34:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 294 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:00 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 20.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:01 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:36325 with 6.3 GB RAM, BlockManagerId(2, algo-1, 36325, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:55 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1104@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:55 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:55 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:55 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:57 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:57 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:57 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:57 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:57 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:59 INFO client.TransportClientFactory: Successfully created connection to /10.0.130.83:46483 after 245 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:59 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:59 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:59 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:59 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:34:59 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:00 INFO client.TransportClientFactory: Successfully created connection to /10.0.130.83:46483 after 9 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:00 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/blockmgr-54413e32-40b6-404e-8161-5b5793a1e1cc\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:00 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GB\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2870 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2876 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67) finished in 3.106 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:00 INFO codegen.CodeGenerator: Code generated in 390.774173 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO codegen.CodeGenerator: Code generated in 29.406744 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO codegen.CodeGenerator: Code generated in 12.132434 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO codegen.CodeGenerator: Code generated in 17.110739 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 30 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:01 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:02 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 4445 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:02 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 4402 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 3)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 3.0 in stage 2.0 (TID 5)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 5.0 in stage 2.0 (TID 7)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 7.0 in stage 2.0 (TID 9)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 42 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:02 INFO codegen.CodeGenerator: Code generated in 261.834229 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.SparkContext: Starting job: collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.DAGScheduler: Got job 1 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67) with 11 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.6 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.0 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.130.83:42973 (size: 11.0 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.DAGScheduler: Submitting 11 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO cluster.YarnScheduler: Adding task set 2.0 with 11 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 6, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 2.0 (TID 8, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 2.0 (TID 9, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:37179 (size: 11.0 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:37179 in memory (size: 10.1 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.130.83:42973 in memory (size: 10.1 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 2.0 (TID 10, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 2.0 (TID 11, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 507 ms on algo-2 (executor 1) (1/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 2.0 (TID 12, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 561 ms on algo-2 (executor 1) (2/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 2.0 (TID 11) in 59 ms on algo-2 (executor 1) (3/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 2.0 (TID 10) in 103 ms on algo-2 (executor 1) (4/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 2.0 (TID 9) in 559 ms on algo-2 (executor 1) (5/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 2.0 (TID 7) in 562 ms on algo-2 (executor 1) (6/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:03 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 2.0 (TID 12) in 34 ms on algo-2 (executor 1) (7/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:04 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:36325 (size: 11.0 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.130.83:46483\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO executor.Executor: Starting executor ID 2 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36325.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO netty.NettyBlockTransferService: Server created on algo-1:36325\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-1, 36325, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-1, 36325, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:01 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, algo-1, 36325, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 2.0 in stage 2.0 (TID 4)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 6.0 in stage 2.0 (TID 8)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 4.0 in stage 2.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:04 INFO client.TransportClientFactory: Successfully created connection to /10.0.130.83:42973 after 11 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:04 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:05 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m03-17 09:35 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1742204047158_0001.inprogress\u001b[0m\n",
      "\u001b[34m03-17 09:35 root         INFO     copying /tmp/spark-events/application_1742204047158_0001.inprogress to /opt/ml/processing/spark-events/application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 5178 ms on algo-1 (executor 2) (8/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5190 ms on algo-1 (executor 2) (9/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 2.0 (TID 8) in 5180 ms on algo-1 (executor 2) (10/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 6) in 5198 ms on algo-1 (executor 2) (11/11)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO scheduler.DAGScheduler: ResultStage 2 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67) finished in 5.237 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO scheduler.DAGScheduler: Job 1 finished: collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:67, took 5.292040 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: double (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- order_in_session: long (nullable = true)\n",
      " |-- purchased_items: long (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- interaction_value: long (nullable = true)\n",
      " |-- cumsum_interactions: long (nullable = true)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:37179 in memory (size: 11.0 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.130.83:42973 in memory (size: 11.0 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:08 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:36325 in memory (size: 11.0 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 436 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:04 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 61 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 78 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 77 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 114 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:07 INFO codegen.CodeGenerator: Code generated in 1371.892606 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:07 INFO codegen.CodeGenerator: Code generated in 20.703069 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:08 INFO codegen.CodeGenerator: Code generated in 51.791725 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO codegen.CodeGenerator: Code generated in 126.666962 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 303.1 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.SparkContext: Created broadcast 3 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.DAGScheduler: Registering RDD 8 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.130.83:42973 (size: 7.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO cluster.YarnScheduler: Adding task set 3.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 13, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 14, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:37179 (size: 7.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:36325 (size: 7.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:09 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 14) in 406 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO codegen.CodeGenerator: Code generated in 45.750201 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Finished task 7.0 in stage 2.0 (TID 9). 3219 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Finished task 3.0 in stage 2.0 (TID 5). 3184 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Finished task 5.0 in stage 2.0 (TID 7). 3219 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 3). 3227 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 8.0 in stage 2.0 (TID 10)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Finished task 8.0 in stage 2.0 (TID 10). 3150 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 9.0 in stage 2.0 (TID 11)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Finished task 9.0 in stage 2.0 (TID 11). 3176 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Running task 10.0 in stage 2.0 (TID 12)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:03 INFO executor.Executor: Finished task 10.0 in stage 2.0 (TID 12). 3193 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 13) in 1488 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 1.537 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:10 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO codegen.CodeGenerator: Code generated in 33.835033 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:08 INFO executor.Executor: Finished task 2.0 in stage 2.0 (TID 4). 3236 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:08 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 3236 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:08 INFO executor.Executor: Finished task 6.0 in stage 2.0 (TID 8). 3236 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:08 INFO executor.Executor: Finished task 4.0 in stage 2.0 (TID 6). 3236 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:08 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:08 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 7290 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:09 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:09 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:09 INFO client.TransportClientFactory: Successfully created connection to algo-2/10.0.164.134:37179 after 21 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:09 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 216 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:10 INFO codegen.CodeGenerator: Code generated in 62.29014 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:10 INFO datasources.FileScanRDD: TID: 13 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:10 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:10 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:10 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:10 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.6 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:37179 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 190 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.217 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO scheduler.DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.236465 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/[Loggs] Total rows in input: 502 --- [Row(event_type='page_view'), Row(event_type='add_to_cart'), Row(event_type='purchase'), Row(event_type='click'), Row(event_type='scroll')]\u001b[0m\n",
      "\u001b[34m[Loggs][Filter and Aggregate Order Data]\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:37179 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:37179 in memory (size: 7.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:36325 in memory (size: 7.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.130.83:42973 in memory (size: 7.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:11 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 14)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 34 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO codegen.CodeGenerator: Code generated in 67.576977 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 23 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:09 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 14). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 15)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 15 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:12 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:12 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:12 INFO datasources.FileSourceStrategy: Output Data Schema: struct<event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:12 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:12 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:12 INFO codegen.CodeGenerator: Code generated in 51.889586 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 303.1 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO spark.SparkContext: Created broadcast 6 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Registering RDD 14 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Got map stage job 4 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.6 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.130.83:42973 (size: 7.7 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO cluster.YarnScheduler: Adding task set 6.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 17, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:37179 (size: 7.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:36325 (size: 7.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 17) in 192 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 16) in 495 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.543 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Got job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.6 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 18, algo-1, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:36325 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 18) in 167 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.212 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:13 INFO scheduler.DAGScheduler: Job 5 finished: count at NativeMethodAccessorImpl.java:0, took 0.232287 s\u001b[0m\n",
      "\u001b[34m[Loggs]Order events count: 11\u001b[0m\n",
      "\u001b[34m03/17 09:35:10 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 13). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 31 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO codegen.CodeGenerator: Code generated in 92.305452 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO datasources.FileScanRDD: TID: 16 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 76 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 16). 1808 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 18)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:37179 in memory (size: 7.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:36325 in memory (size: 7.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.130.83:42973 in memory (size: 7.7 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned shuffle 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:36325 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 1 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO client.TransportClientFactory: Successfully created connection to algo-1/10.0.130.83:36325 after 6 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 41 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO codegen.CodeGenerator: Code generated in 26.233618 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:11 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 17)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 15 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO codegen.CodeGenerator: Code generated in 53.545664 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 23 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:14 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#30),(event_type#30 = click)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO datasources.FileSourceStrategy: Output Data Schema: struct<event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO execution.FileSourceScanExec: Pushed Filters: EqualTo(none,click),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO codegen.CodeGenerator: Code generated in 66.45753 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 303.1 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.SparkContext: Created broadcast 9 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Registering RDD 20 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 14.2 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.130.83:42973 (size: 7.5 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO cluster.YarnScheduler: Adding task set 9.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 19, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 20, algo-1, executor 2, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:37179 (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:36325 (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 19) in 154 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 20) in 233 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.247 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 7.6 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 288\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 287\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 286\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 289\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:37179 in memory (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.130.83:42973 in memory (size: 7.5 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 21, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:36325 in memory (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 285\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:37179 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:15 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 21) in 49 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO scheduler.DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.120 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO scheduler.DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.131750 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlo[Loggs]Click events count: 108\u001b[0m\n",
      "\u001b[34mgs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 22 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 1 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 28 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO codegen.CodeGenerator: Code generated in 24.983571 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:13 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 18). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 20)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 17 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 14.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO codegen.CodeGenerator: Code generated in 29.451407 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO datasources.FileScanRDD: TID: 20 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 297\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 320\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 319\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 317\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 294\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 304\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 291\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 315\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 300\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 316\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 311\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 309\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 295\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:37179 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 303\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 298\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 290\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned shuffle 3\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 318\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 301\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 302\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 307\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 308\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 314\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 305\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 292\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 310\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 299\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 293\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 312\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 296\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 313\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 306\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:16 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#170),(event_type#170 = click),isnotnull(customer_id#168L)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: bigint, event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click),IsNotNull(customer_id)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO execution.FileSourceScanExec: Pushed Filters: EqualTo(none,click),IsNotNull(none),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO codegen.CodeGenerator: Code generated in 19.784631 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 303.1 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO spark.SparkContext: Created broadcast 12 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO spark.SparkContext: Starting job: run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.DAGScheduler: Got job 8 (run at Executors.java:511) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (run at Executors.java:511)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[26] at run at Executors.java:511), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.0 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.130.83:42973 (size: 6.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[26] at run at Executors.java:511) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO cluster.YarnScheduler: Adding task set 12.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 22, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8297 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 23, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8302 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:36325 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:17 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:37179 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 23) in 203 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 22) in 224 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: ResultStage 12 (run at Executors.java:511) finished in 0.232 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Job 8 finished: run at Executors.java:511, took 0.237313 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 16.882743 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 1030.3 KB, free 1027.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 381\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 382\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 379\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 397\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 393\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 380\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 378\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 386\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 377\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 390\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 385\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 392\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 394\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 399\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 398\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 323\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 401\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 384\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 400\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 391\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 396\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 389\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1242.0 B, free 1027.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.130.83:42973 (size: 1242.0 B, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:37179 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.SparkContext: Created broadcast 14 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:36325 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.130.83:42973 in memory (size: 6.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 388\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 387\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 395\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.ContextCleaner: Cleaned accumulator 383\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:13 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 17). 1808 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 19)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 14.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO codegen.CodeGenerator: Code generated in 17.241943 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO datasources.FileScanRDD: TID: 19 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 22 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 19). 1808 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 21)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 22.087379 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 303.1 KB, free 1026.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1026.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.SparkContext: Created broadcast 15 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Registering RDD 29 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Got map stage job 9 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 13 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 16.0 KB, free 1026.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.3 KB, free 1026.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.130.83:42973 (size: 8.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO cluster.YarnScheduler: Adding task set 13.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 24, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 25, algo-1, executor 2, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:37179 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:36325 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:37179 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:36325 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 24) in 299 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:18 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 21 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:15 INFO executor.Executor: Finished task 1.0 in stage 9.0 (TID 20). 1851 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 17 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:17 INFO codegen.CodeGenerator: Code generated in 37.075119 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO datasources.FileScanRDD: TID: 22 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 14.888985 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 22). 1709 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 25) in 573 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (count at NativeMethodAccessorImpl.java:0) finished in 0.593 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 351.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 28.910366 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 48.215483 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:15 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:16 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:16 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 1 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:16 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:16 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 21). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:17 INFO executor.Executor: Running task 1.0 in stage 12.0 (TID 23)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 27 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:17 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 28.347719 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO datasources.FileScanRDD: TID: 23 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 20.878829 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 27 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO executor.Executor: Finished task 1.0 in stage 12.0 (TID 23). 1595 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 24)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 15 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 40.848078 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 22 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 24.720031 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO datasources.FileScanRDD: TID: 24 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 10.245004 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 429\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 451\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 417\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 448\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 439\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 410\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 443\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 454\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 419\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 432\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 363\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 406\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 402\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 366\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 411\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 364\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 421\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 414\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 420\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 409\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 431\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 357\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 436\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 371\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 446\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 441\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 403\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 433\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 440\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 360\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 404\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 445\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 435\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 413\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 442\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 408\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 427\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 356\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 365\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 428\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 415\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 361\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 434\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 425\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 369\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 412\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:37179 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.130.83:42973 in memory (size: 8.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:36325 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 422\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 430\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 358\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 449\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 368\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 362\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 444\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 437\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 450\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 447\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 416\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 367\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 438\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 418\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 359\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 426\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 355\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.ContextCleaner: Cleaned accumulator 370\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: Registering RDD 34 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 6 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 31.5 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 14.7 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.130.83:42973 (size: 14.7 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO cluster.YarnScheduler: Adding task set 15.0 with 6 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 26, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 27, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 28, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 15.0 (TID 29, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 15.0 (TID 30, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 15.0 (TID 31, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:37179 (size: 14.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:36325 (size: 14.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 28) in 500 ms on algo-2 (executor 1) (1/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 26) in 501 ms on algo-2 (executor 1) (2/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:19 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 15.0 (TID 30) in 503 ms on algo-2 (executor 1) (3/6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 25)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 24 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 124.983626 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 41 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 21.940101 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO datasources.FileScanRDD: TID: 25 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO codegen.CodeGenerator: Code generated in 13.470903 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 26 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 25). 3808 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO executor.Executor: Running task 1.0 in stage 15.0 (TID 27)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO executor.Executor: Running task 3.0 in stage 15.0 (TID 29)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO executor.Executor: Running task 5.0 in stage 15.0 (TID 31)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 14.7 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 33 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 31.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 24 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 29 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 23 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:18 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 24). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 26)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.Executor: Running task 2.0 in stage 15.0 (TID 28)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.Executor: Running task 4.0 in stage 15.0 (TID 30)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 14.7 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 18 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 31.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 15.0 (TID 31) in 1028 ms on algo-1 (executor 2) (4/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 15.0 (TID 29) in 1043 ms on algo-1 (executor 2) (5/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 27) in 1052 ms on algo-1 (executor 2) (6/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 1.088 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 13.755668 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 7.6 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 32, algo-1, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:36325 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 32) in 94 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: ResultStage 18 (count at NativeMethodAccessorImpl.java:0) finished in 0.103 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO scheduler.DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.109634 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/co[Loggs]aggregate_features - grouped_df count: 89\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 374\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 494\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 344\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 498\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 495\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 469\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 487\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 328\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 481\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 496\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 324\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#170),(event_type#170 = click),isnotnull(customer_id#168L)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: bigint, event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click),IsNotNull(customer_id)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:36325 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 456\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 491\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 375\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 513\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 508\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 512\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 325\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 457\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 486\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 459\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 516\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 336\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 330\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 523\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 485\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 473\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 505\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 482\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 460\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO execution.FileSourceScanExec: Pushed Filters: EqualTo(none,click),IsNotNull(none),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 352\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 524\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 525\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 326\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 475\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 483\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 376\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 507\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 333\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO spark.ContextCleaner: Cleaned accumulator 345\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-2:37179 in memory (size: 14.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:20 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:36325 in memory (size: 14.7 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.130.83:42973 in memory (size: 14.7 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 518\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 519\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 343\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 342\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 522\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 407\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 466\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 341\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 509\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 337\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 478\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:37179 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.130.83:42973 in memory (size: 1242.0 B, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34mntainer_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 37.895817 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 43.11115 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 51.869242 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 47.591685 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 30.077789 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 16.4911 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 17.371594 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 47.481488 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 25.09699 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO executor.Executor: Finished task 5.0 in stage 15.0 (TID 31). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO executor.Executor: Finished task 3.0 in stage 15.0 (TID 29). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO executor.Executor: Finished task 1.0 in stage 15.0 (TID 27). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 32)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 19 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 303.1 KB, free 1027.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1027.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:36325 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.SparkContext: Created broadcast 19 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 332\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 470\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 472\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 339\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 351\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 497\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 463\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned shuffle 5\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.SparkContext: Starting job: run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Got job 12 (run at Executors.java:511) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (run at Executors.java:511)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[40] at run at Executors.java:511), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 12.0 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.130.83:42973 (size: 6.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[40] at run at Executors.java:511) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO cluster.YarnScheduler: Adding task set 19.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 33, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8297 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 34, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8302 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:37179 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 500\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 461\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 350\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 488\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 492\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 455\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 510\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 453\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 327\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 464\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 354\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 489\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 452\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 479\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 458\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 493\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 353\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 335\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 520\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 503\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 502\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 476\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 467\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 521\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 468\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 340\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 331\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 423\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 471\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 465\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 480\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 372\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 527\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 515\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 517\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 373\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 477\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 484\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 334\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 506\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 346\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 348\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 474\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 405\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 424\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 322\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 338\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 526\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 329\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 349\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 347\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 462\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 499\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 514\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned shuffle 4\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 490\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 504\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 321\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 501\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:36325 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 34) in 126 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 33) in 182 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: ResultStage 19 (run at Executors.java:511) finished in 0.206 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Job 12 finished: run at Executors.java:511, took 0.225271 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 1030.3 KB, free 1027.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 1242.0 B, free 1027.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.130.83:42973 (size: 1242.0 B, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.SparkContext: Created broadcast 21 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 303.1 KB, free 1026.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1026.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.SparkContext: Created broadcast 22 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Registering RDD 43 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Got map stage job 13 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[43] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 16.0 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.3 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.130.83:42973 (size: 8.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[43] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO cluster.YarnScheduler: Adding task set 20.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 35, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 36, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:36325 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-2:37179 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:36325 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:37179 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 35) in 153 ms on algo-1 (executor 2) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 36) in 169 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (count at NativeMethodAccessorImpl.java:0) finished in 0.180 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 351.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 636\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 574\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 572\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 590\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 639\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 589\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 563\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 588\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 601\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 616\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 562\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 600\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 620\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 587\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 634\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 647\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 633\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 621\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 604\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 638\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 597\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 623\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 642\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 564\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 624\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 594\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 640\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 645\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 635\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 661\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 576\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 615\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 603\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 625\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 643\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 626\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 632\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 591\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 570\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 610\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 573\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-2:37179 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.130.83:42973 in memory (size: 8.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:36325 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-2:37179 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.130.83:42973 in memory (size: 6.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:36325 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Registering RDD 48 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Got map stage job 14 (count at NativeMethodAccessorImpl.java:0) with 6 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[48] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 565\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 652\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 655\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 654\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 567\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 649\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 644\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 641\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 637\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 609\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 585\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 596\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 577\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 593\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 566\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 530\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 569\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 658\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 606\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 602\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 656\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 605\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 653\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 611\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 586\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 598\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 622\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 584\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 627\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 578\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 617\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 613\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 648\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 595\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 628\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 646\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 592\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 568\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 599\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 571\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 650\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 607\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 619\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 629\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 575\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 618\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 608\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 651\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.ContextCleaner: Cleaned accumulator 657\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 31.5 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 14.8 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.130.83:42973 (size: 14.8 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[48] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO cluster.YarnScheduler: Adding task set 22.0 with 6 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 37, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 38, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 22.0 (TID 39, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 22.0 (TID 40, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 22.0 (TID 41, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 22.0 (TID 42, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-2:37179 (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:36325 (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 22.0 (TID 39) in 64 ms on algo-2 (executor 1) (1/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 22.0 (TID 41) in 67 ms on algo-2 (executor 1) (2/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 37) in 72 ms on algo-2 (executor 1) (3/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 22.0 (TID 42) in 106 ms on algo-1 (executor 2) (4/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 22.0 (TID 40) in 117 ms on algo-1 (executor 2) (5/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 38) in 145 ms on algo-1 (executor 2) (6/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0) finished in 0.160 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:21 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: Got job 15 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: Final stage: ResultStage 25 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[51] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 7.6 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 3 local blocks and 3 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO codegen.CodeGenerator: Code generated in 28.152675 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:20 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 32). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 21 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO datasources.FileScanRDD: TID: 33 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 26 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 33). 1709 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 35)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO datasources.FileScanRDD: TID: 35 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 35). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 1.0 in stage 22.0 (TID 38)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 43, algo-1, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:36325 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 43) in 65 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: ResultStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.074 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO scheduler.DAGScheduler: Job 15 finished: count at NativeMethodAccessorImpl.java:0, took 0.081350 s\u001b[0m\n",
      "\u001b[34m[[Loggs][Write Aggregated Features to S3] 89\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 717\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 702\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 550\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 631\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 541\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 730\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 540\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 688\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 695\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 725\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 724\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 722\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 665\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 614\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 709\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 734\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:36325 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 545\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 697\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 536\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 700\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 698\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 662\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 704\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 676\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 533\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 732\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned shuffle 7\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 548\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 678\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 707\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 674\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 696\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 721\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 687\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 706\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 733\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 683\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 669\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 660\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned shuffle 6\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 543\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 681\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 727\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 705\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 693\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 560\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 537\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 715\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 708\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 714\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 539\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 551\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 612\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 553\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 582\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 18.403117 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 14.577151 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 9.822049 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 11.685803 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 9.74074 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 10.850316 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 12.514715 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 10.173053 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO codegen.CodeGenerator: Code generated in 15.302171 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.Executor: Finished task 2.0 in stage 15.0 (TID 28). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 26). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:19 INFO executor.Executor: Finished task 4.0 in stage 15.0 (TID 30). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 1.0 in stage 19.0 (TID 34)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 19 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO datasources.FileScanRDD: TID: 34 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 27 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 1.0 in stage 19.0 (TID 34). 1638 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 1.0 in stage 20.0 (TID 36)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 20 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 16 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO datasources.FileScanRDD: TID: 36 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 18 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 1.0 in stage 20.0 (TID 36). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 0.0 in stage 22.0 (TID 37)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 2.0 in stage 22.0 (TID 39)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 4.0 in stage 22.0 (TID 41)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 14.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 12 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 31.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-2:37179 in memory (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:36325 in memory (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.130.83:42973 in memory (size: 14.8 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 580\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 701\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 529\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 675\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 663\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 538\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 689\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 549\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 558\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 555\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 726\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 690\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 559\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 581\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 731\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 692\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 546\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 711\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 556\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 723\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 557\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 535\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 544\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 561\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 679\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 532\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 664\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 673\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 659\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 685\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 699\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 677\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 712\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 686\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 710\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.130.83:42973 in memory (size: 1242.0 B, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:36325 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-2:37179 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 666\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 583\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 579\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string, total_amount: double ... 2 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#170),(event_type#170 = click),isnotnull(customer_id#168L)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click),IsNotNull(customer_id)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 542\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 684\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 670\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 552\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 729\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 671\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 720\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 531\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 672\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 668\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 680\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 547\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 554\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 716\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 713\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 528\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 718\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 703\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 728\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 694\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 691\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 630\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 534\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 682\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 719\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO spark.ContextCleaner: Cleaned accumulator 667\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:22 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(none),EqualTo(none,click),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 3.0 in stage 22.0 (TID 40)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Running task 5.0 in stage 22.0 (TID 42)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 14.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 21 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 31.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 5.0 in stage 22.0 (TID 42). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 3.0 in stage 22.0 (TID 40). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 1.0 in stage 22.0 (TID 38). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 43)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 2.0 in stage 22.0 (TID 39). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 4.0 in stage 22.0 (TID 41). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:21 INFO executor.Executor: Finished task 0.0 in stage 22.0 (TID 37). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 44)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 12 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 14.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 17.407986 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO datasources.FileScanRDD: TID: 44 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 17 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 44). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 1.0 in stage 27.0 (TID 47)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.9 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 17 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 11.51326 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 15.046321 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO datasources.FileScanRDD: TID: 47 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 9.253169 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 16 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Finished task 1.0 in stage 27.0 (TID 47). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 48)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 6.0 in stage 29.0 (TID 54)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 2.0 in stage 29.0 (TID 50)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 10 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 4.0 in stage 29.0 (TID 52)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 12.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 20 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 25.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Don't have map[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:45.740+0000: [GC (Allocation Failure) [PSYoungGen: 57856K->4478K(67072K)] 57856K->4486K(220160K), 0.0053251 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:46.083+0000: [GC (Allocation Failure) [PSYoungGen: 62334K->4678K(67072K)] 62342K->4694K(220160K), 0.0068050 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:46.405+0000: [GC (Allocation Failure) [PSYoungGen: 62534K->4548K(67072K)] 62550K->4572K(220160K), 0.0039838 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:46.819+0000: [GC (Allocation Failure) [PSYoungGen: 62404K->5415K(124928K)] 62428K->5447K(278016K), 0.0080853 secs] [Times: user=0.02 sys=0.01, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:47.376+0000: [GC (Allocation Failure) [PSYoungGen: 121127K->6719K(124928K)] 121159K->6759K(278016K), 0.0096710 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:47.959+0000: [GC (Allocation Failure) [PSYoungGen: 122431K->6644K(227840K)] 122471K->23944K(380928K), 0.0173771 secs] [Times: user=0.04 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:48.025+0000: [GC (Metadata GC Threshold) [PSYoungGen: 20709K->3022K(238080K)] 38009K->23135K(391168K), 0.0111734 secs] [Times: user=0.04 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:48.036+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3022K->0K(238080K)] [ParOldGen: 20112K->22149K(101376K)] 23135K->22149K(339456K), [Metaspace: 20938K->20938K(1067008K)], 0.0483527 secs] [Times: user=0.10 sys=0.00, real=0.05 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:48.993+0000: [GC (Allocation Failure) [PSYoungGen: 221184K->8347K(397824K)] 243333K->46888K(499200K), 0.0214143 secs] [Times: user=0.05 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:49.704+0000: [GC (Metadata GC Threshold) [PSYoungGen: 163479K->9648K(451584K)] 202021K->48198K(552960K), 0.0142373 secs] [Times: user=0.03 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:34:49.718+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 9648K->0K(451584K)] [ParOldGen: 38549K->47361K(160768K)] 48198K->47361K(612352K), [Metaspace: 34875K->34875K(1079296K)], 0.0758137 secs] [Times: user=0.22 sys=0.01, real=0.08 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:35:01.196+0000: [GC (Allocation Failure) [PSYoungGen: 435200K->16365K(566784K)] 482561K->81495K(727552K), 0.0436375 secs] [Times: user=0.04 sys=0.02, real=0.05 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:35:14.280+0000: [GC (Allocation Failure) [PSYoungGen: 566765K->21515K(575488K)] 631895K->109059K(736256K), 0.0381387 secs] [Times: user=0.04 sys=0.03, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stdout] 2025-03-17T09:35:24.966+0000: [GC (Allocation Failure) [PSYoungGen: 525627K->23807K(718848K)] 613171K->439039K(1143808K), 0.1726038 secs] [Times: user=0.19 sys=0.25, real=0.17 secs] outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 12.009468 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 12.613329 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 17.766962 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 7.886236 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 8.192626 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 18.744655 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 12.854087 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 16.986088 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 9.442901 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 8.096732 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 4.0 in stage 29.0 (TID 52). 2789 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 48). 2769 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 6.0 in stage 29.0 (TID 54). 2788 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 2.0 in stage 29.0 (TID 50). 2771 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 8.0 in stage 29.0 (TID 56)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 10.0 in stage 29.0 (TID 58)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 9.0 in stage 29.0 (TID 57)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 11.0 in stage 29.0 (TID 59)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 11.0 in stage 29.0 (TID 59). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:23 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:23 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:23 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:23 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 737\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 739\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 39.21108 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 303.1 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.SparkContext: Created broadcast 26 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Registering RDD 54 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Got map stage job 16 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 26 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 14.1 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.130.83:42973 (size: 7.5 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO cluster.YarnScheduler: Adding task set 26.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 44, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 26.0 (TID 45, algo-1, executor 2, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-2:37179 (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:36325 (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 66.15211 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 303.1 KB, free 1027.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1027.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.SparkContext: Created broadcast 28 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Registering RDD 57 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Got map stage job 17 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 27 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[57] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.0 KB, free 1027.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.9 KB, free 1027.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.130.83:42973 (size: 7.9 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[57] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO cluster.YarnScheduler: Adding task set 27.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 46, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 47, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 44) in 153 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:36325 (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-2:37179 (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 26.0 (TID 45) in 224 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: ShuffleMapStage 26 (save at NativeMethodAccessorImpl.java:0) finished in 0.236 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 129.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 47) in 145 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 56.466929 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 46) in 366 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (save at NativeMethodAccessorImpl.java:0) finished in 0.376 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 791\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.SparkContext: Starting job: run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 827\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 819\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Got job 18 (run at Executors.java:511) with 16 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 29 (run at Executors.java:511)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[62] at run at Executors.java:511), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-2:37179 in memory (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 25.4 KB, free 1027.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.130.83:42973 in memory (size: 7.9 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 12.3 KB, free 1027.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.130.83:42973 (size: 12.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ResultStage 29 (MapPartitionsRDD[62] at run at Executors.java:511) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO cluster.YarnScheduler: Adding task set 29.0 with 16 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 48, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 29.0 (TID 49, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 29.0 (TID 50, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 29.0 (TID 51, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 29.0 (TID 52, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 29.0 (TID 53, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 29.0 (TID 54, algo-2, executor 1, partition 6, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 29.0 (TID 55, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:36325 in memory (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-2:37179 (size: 12.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 839\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 778\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 806\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 810\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 820\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 841\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 830\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 805\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 824\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 825\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 837\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 775\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 808\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.130.83:42973 in memory (size: 7.5 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:36325 in memory (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-2:37179 in memory (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 814\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 803\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 777\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 817\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 790\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 847\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 818\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 828\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 834\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 796\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 822\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 789\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 823\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 792\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 809\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 838\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 787\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 842\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 812\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 795\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 816\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 788\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 779\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 833\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 836\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 800\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 807\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 840\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 797\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 843\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 804\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 829\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 831\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 776\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 832\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 813\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 821\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 798\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 801\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 835\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 844\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 793\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 799\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 811\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 845\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 815\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 826\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.ContextCleaner: Cleaned accumulator 802\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:36325 (size: 12.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 7, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 3 local blocks and 3 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:22 INFO executor.Executor: Finished task 0.0 in stage 25.0 (TID 43). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 1.0 in stage 26.0 (TID 45)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 14.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 73.09522 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO datasources.FileScanRDD: TID: 45 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 46)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.9 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 47.733163 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Finished task 1.0 in stage 26.0 (TID 45). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 29.272841 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO datasources.FileScanRDD: TID: 46 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO codegen.CodeGenerator: Code generated in 32.557375 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 20 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 46). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 29.0 (TID 56, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 29.0 (TID 57, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 29.0 (TID 58, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 29.0 (TID 59, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 29.0 (TID 52) in 786 ms on algo-2 (executor 1) (1/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 29.0 (TID 50) in 788 ms on algo-2 (executor 1) (2/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 29.0 (TID 54) in 784 ms on algo-2 (executor 1) (3/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 48) in 791 ms on algo-2 (executor 1) (4/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 29.0 (TID 60, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 29.0 (TID 53) in 796 ms on algo-1 (executor 2) (5/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 29.0 (TID 61, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 29.0 (TID 49) in 804 ms on algo-1 (executor 2) (6/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 29.0 (TID 62, algo-1, executor 2, partition 14, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 29.0 (TID 51) in 809 ms on algo-1 (executor 2) (7/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 29.0 (TID 63, algo-1, executor 2, partition 15, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 29.0 (TID 55) in 815 ms on algo-1 (executor 2) (8/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 29.0 (TID 59) in 41 ms on algo-2 (executor 1) (9/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 29.0 (TID 57) in 48 ms on algo-2 (executor 1) (10/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 29.0 (TID 58) in 51 ms on algo-2 (executor 1) (11/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 29.0 (TID 56) in 57 ms on algo-2 (executor 1) (12/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 29.0 (TID 61) in 64 ms on algo-1 (executor 2) (13/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 29.0 (TID 63) in 50 ms on algo-1 (executor 2) (14/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 29.0 (TID 60) in 87 ms on algo-1 (executor 2) (15/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 29.0 (TID 62) in 88 ms on algo-1 (executor 2) (16/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.DAGScheduler: ResultStage 29 (run at Executors.java:511) finished in 0.910 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO scheduler.DAGScheduler: Job 18 finished: run at Executors.java:511, took 0.916976 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 1030.3 KB, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 1818.0 B, free 1026.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.130.83:42973 (size: 1818.0 B, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO spark.SparkContext: Created broadcast 31 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 20.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 27.557745 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 71.457164 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 28.412334 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 16.076141 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 884\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 866\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 781\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 870\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 873\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 849\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 882\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 864\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 852\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 875\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 862\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 867\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 872\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 785\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 885\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 878\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 851\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 887\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 876\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 863\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 865\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 850\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 869\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 868\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-2:37179 in memory (size: 12.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:36325 in memory (size: 12.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.130.83:42973 in memory (size: 12.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 848\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 877\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 780\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 880\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 881\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 871\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 782\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 883\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 879\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 784\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 886\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 783\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.ContextCleaner: Cleaned accumulator 874\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/appli[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:54.800+0000: [GC (Allocation Failure) [PSYoungGen: 57856K->4006K(67072K)] 57856K->4006K(220160K), 0.0076101 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:55.552+0000: [GC (Allocation Failure) [PSYoungGen: 61862K->4452K(67072K)] 61862K->4460K(220160K), 0.0110273 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:56.305+0000: [GC (Allocation Failure) [PSYoungGen: 62308K->4692K(67072K)] 62316K->4708K(220160K), 0.0078369 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:57.033+0000: [GC (Allocation Failure) [PSYoungGen: 62548K->5289K(67072K)] 62564K->5313K(220160K), 0.0181728 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:57.652+0000: [GC (Allocation Failure) [PSYoungGen: 63145K->5525K(67072K)] 63169K->5557K(220160K), 0.0102027 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:58.365+0000: [GC (Allocation Failure) [PSYoungGen: 63381K->5696K(121344K)] 63413K->5728K(274432K), 0.0223160 secs] [Times: user=0.02 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:59.629+0000: [GC (Metadata GC Threshold) [PSYoungGen: 116537K->5438K(121856K)] 116569K->24288K(274944K), 0.0502804 secs] [Times: user=0.06 sys=0.01, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:34:59.679+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 5438K->0K(121856K)] [ParOldGen: 18850K->23983K(109568K)] 24288K->23983K(231424K), [Metaspace: 20953K->20953K(1067008K)], 0.0696666 secs] [Times: user=0.07 sys=0.01, real=0.07 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:35:00.813+0000: [GC (Allocation Failure) [PSYoungGen: 115200K->3730K(198144K)] 139183K->44106K(307712K), 0.0223530 secs] [Times: user=0.05 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:35:02.550+0000: [GC (Allocation Failure) [PSYoungGen: 197778K->6627K(234496K)] 238154K->47838K(344064K), 0.0850002 secs] [Times: user=0.06 sys=0.00, real=0.08 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:35:04.338+0000: [GC (Metadata GC Threshold) [PSYoungGen: 176235K->7674K(287232K)] 217446K->66082K(396800K), 0.0467554 secs] [Times: user=0.06 sys=0.02, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:35:04.385+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 7674K->0K(287232K)] [ParOldGen: 58407K->62550K(186880K)] 66082K->62550K(474112K), [Metaspace: 35074K->35070K(1079296K)], 0.1214328 secs] [Times: user=0.15 sys=0.01, real=0.12 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:35:07.855+0000: [GC (Allocation Failure) [PSYoungGen: 279552K->8185K(287744K)] 342102K->76824K(474624K), 0.0277091 secs] [Times: user=0.04 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stdout] 2025-03-17T09:35:18.979+0000: [GC (Allocation Failure) [PSYoungGen: 272880K->8235K(372736K)] 341518K->115818K(559616K), 0.0415030 secs] [Times: user=0.08 sys=0.02, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/location_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 1.0 in stage 29.0 (TID 49)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 7.0 in stage 29.0 (TID 55)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 5.0 in stage 29.0 (TID 53)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 10 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO executor.Executor: Running task 3.0 in stage 29.0 (TID 51)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 12.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 49 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:24 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 25.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 26.779368 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 18.185231 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 31.955864 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 27.766568 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 14.302456 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 20.182485 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 14.551085 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 29.135315 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 16.34243 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO codegen.CodeGenerator: Code generated in 14.655284 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 5.0 in stage 29.0 (TID 53). 2720 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 3.0 in stage 29.0 (TID 51). 2761 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 1.0 in stage 29.0 (TID 49). 2789 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO scheduler.DAGScheduler: Got job 19 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (CoalescedRDD[78] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 208.0 KB, free 1026.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 76.9 KB, free 1026.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.130.83:42973 (size: 76.9 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (CoalescedRDD[78] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 64, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8418 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-2:37179 (size: 76.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:26 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:27 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-2:37179 (size: 1818.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 9.0 in stage 29.0 (TID 57). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 10.0 in stage 29.0 (TID 58). 2738 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 8.0 in stage 29.0 (TID 56). 2746 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO executor.Executor: Running task 0.0 in stage 31.0 (TID 64)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 76.9 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 21 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 208.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO codegen.CodeGenerator: Code generated in 16.48511 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO codegen.CodeGenerator: Code generated in 8.002597 ms\u001b[0m\n",
      "\u001b[34m03-17 09:35 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1742204047158_0001.inprogress\u001b[0m\n",
      "\u001b[34m03-17 09:35 root         INFO     copying /tmp/spark-events/application_1742204047158_0001.inprogress to /opt/ml/processing/spark-events/application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:26 INFO codegen.CodeGenerator: Code generated in 7.72678 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 7.750492 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 6.499962 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 7.480413 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 7.623607 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 7.408522 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 15.826269 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 7.924584 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 8.820782 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 6.04521 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 8.804942 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 7.240356 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 6.314826 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 7.37601 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 6.079576 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 17.065898 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 1818.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 9 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 9.7 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 8.766474 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO codegen.CodeGenerator: Code generated in 8.65381 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 64) in 2207 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:28 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:28 INFO scheduler.DAGScheduler: ResultStage 31 (save at NativeMethodAccessorImpl.java:0) finished in 2.302 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:28 INFO scheduler.DAGScheduler: Job 19 finished: save at NativeMethodAccessorImpl.java:0, took 2.310115 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileFormatWriter: Write Job 5b8724c2-7ef1-4d3b-aaa4-79017c1161e4 committed.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileFormatWriter: Finished processing stats for write job 5b8724c2-7ef1-4d3b-aaa4-79017c1161e4.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_0000[Loggs][Group Aggregated Features by User]\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 907\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 905\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 910\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 925\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 921\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 912\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 926\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 923\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 911\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 908\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 914\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 916\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 913\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 903\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 918\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 904\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 927\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 924\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 909\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 920\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 906\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 919\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.130.83:42973 in memory (size: 76.9 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-2:37179 in memory (size: 76.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 915\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 922\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.ContextCleaner: Cleaned accumulator 917\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#170),(event_type#170 = click),isnotnull(customer_id#168L)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: bigint, event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click),IsNotNull(customer_id)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO execution.FileSourceScanExec: Pushed Filters: EqualTo(none,click),IsNotNull(none),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 303.1 KB, free 1026.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1026.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO spark.SparkContext: Created broadcast 33 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:29 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Starting job: run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Got job 20 (run at Executors.java:511) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (run at Executors.java:511)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[83] at run at Executors.java:511), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 12.0 KB, free 1026.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1026.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.130.83:42973 (size: 6.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[83] at run at Executors.java:511) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Adding task set 32.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 65, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8297 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 32.0 (TID 66, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8302 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:36325 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m03/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 12.0 in stage 29.0 (TID 60)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 13.0 in stage 29.0 (TID 61)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 14.0 in stage 29.0 (TID 62)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 7.0 in stage 29.0 (TID 55). 2761 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Running task 15.0 in stage 29.0 (TID 63)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 13.0 in stage 29.0 (TID 61). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 15.0 in stage 29.0 (TID 63). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 12.0 in stage 29.0 (TID 60). 2766 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:25 INFO executor.Executor: Finished task 14.0 in stage 29.0 (TID 62). 3169 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-2:37179 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 32.0 (TID 66) in 99 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 65) in 143 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: ResultStage 32 (run at Executors.java:511) finished in 0.149 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Job 20 finished: run at Executors.java:511, took 0.156919 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 1030.3 KB, free 1025.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 1242.0 B, free 1025.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.130.83:42973 (size: 1242.0 B, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Created broadcast 35 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1013\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 988\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 971\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1010\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1009\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 992\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 966\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 994\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1002\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1008\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1003\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 975\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 970\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 986\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 965\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 998\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 963\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 996\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1011\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 976\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 984\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 962\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 991\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1007\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 997\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 990\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 967\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 930\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 974\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1006\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 989\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 999\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 973\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1000\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1005\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 964\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 995\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.130.83:42973 in memory (size: 6.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-2:37179 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:36325 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 303.1 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 977\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1004\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 985\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 972\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 969\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 993\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 978\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 968\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 987\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Created broadcast 36 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Registering RDD 86 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Got map stage job 21 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[86] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 16.0 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 8.3 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.130.83:42973 (size: 8.3 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[86] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Adding task set 33.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 67, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 68, algo-1, executor 2, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-2:37179 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:36325 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-2:37179 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:36325 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 67) in 146 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 68) in 192 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (count at NativeMethodAccessorImpl.java:0) finished in 0.208 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 351.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Registering RDD 91 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Got map stage job 22 (count at NativeMethodAccessorImpl.java:0) with 6 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 35 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[91] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 31.5 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 14.8 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.130.83:42973 (size: 14.8 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[91] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Adding task set 35.0 with 6 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 69, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 35.0 (TID 70, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 35.0 (TID 71, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 35.0 (TID 72, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 35.0 (TID 73, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 35.0 (TID 74, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-2:37179 (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:36325 (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 69) in 57 ms on algo-2 (executor 1) (1/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 35.0 (TID 73) in 60 ms on algo-2 (executor 1) (2/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 35.0 (TID 71) in 63 ms on algo-2 (executor 1) (3/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 35.0 (TID 70) in 90 ms on algo-1 (executor 2) (4/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 35.0 (TID 74) in 120 ms on algo-1 (executor 2) (5/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 35.0 (TID 72) in 123 ms on algo-1 (executor 2) (6/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: ShuffleMapStage 35 (count at NativeMethodAccessorImpl.java:0) finished in 0.138 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Got job 23 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Final stage: ResultStage 38 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[94] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 7.6 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[94] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 75, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-2:37179 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 75) in 35 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: ResultStage 38 (count at NativeMethodAccessorImpl.java:0) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO scheduler.DAGScheduler: Job 23 finished: count at NativeMethodAccessorImpl.java:0, took 0.061677 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_17422040471[Loggs]sorted_df count 89\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1115\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 934\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1058\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1100\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1085\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1126\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1120\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1118\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1072\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 938\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 935\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1105\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 947\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 937\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1097\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1063\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1104\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 957\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1066\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1109\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1095\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1081\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1110\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1119\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1122\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 941\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 936\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.130.83:42973 in memory (size: 14.8 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-2:37179 in memory (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:36325 in memory (size: 14.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1073\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1024\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1080\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1131\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1077\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1035\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 929\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1062\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1051\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 949\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned shuffle 10\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1071\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned shuffle 11\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1015\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1036\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1028\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1021\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1102\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1075\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 943\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1029\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 932\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1012\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1061\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1117\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1121\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1020\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1106\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1044\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 956\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1094\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1128\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1088\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1134\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1049\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 948\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1031\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1038\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 955\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 952\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1123\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 942\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1089\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1016\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1017\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 944\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1033\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1026\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 981\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 950\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1090\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1064\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1045\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1133\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1083\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1040\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 959\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1070\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1116\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1082\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1046\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1023\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1107\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 933\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1052\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1091\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1060\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1032\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 982\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 960\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1076\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1108\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1065\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1022\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1025\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1047\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 931\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 951\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1099\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1103\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1087\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 939\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1112\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1069\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1042\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1039\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1068\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1056\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO spark.ContextCleaner: Cleaned accumulator 1084\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:30 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1078\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 953\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1111\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 928\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1030\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-2:37179 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1074\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1129\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1098\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1050\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1086\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1053\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 945\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1114\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1034\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1101\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1092\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1127\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 946\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.130.83:42973 in memory (size: 8.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-2:37179 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:36325 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1043\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 961\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1055\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 958\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1041\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1125\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1054\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1132\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1096\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 979\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.130.83:42973 in memory (size: 1242.0 B, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-2:37179 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:36325 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m58_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 0.0 in stage 32.0 (TID 65)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 34\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 34 took 17 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO datasources.FileScanRDD: TID: 65 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 0.0 in stage 32.0 (TID 65). 1709 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 1.0 in stage 33.0 (TID 68)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 37\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 37 took 31 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 35\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 35 took 21 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO datasources.FileScanRDD: TID: 68 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 36\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 36 took 26 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 1.0 in stage 33.0 (TID 68). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 1.0 in stage 35.0 (TID 70)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Updating epoch to 11 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 38\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 3.0 in stage 35.0 (TID 72)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 5.0 in stage 35.0 (TID 74)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 14.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 38 took 24 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 31.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1113\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1018\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 940\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 980\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1130\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1079\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1093\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1059\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1037\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 954\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1019\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1067\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1057\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1048\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1124\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1027\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 983\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#170),(event_type#170 = click),isnotnull(customer_id#168L)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: bigint, event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click),IsNotNull(customer_id)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: Pushed Filters: EqualTo(none,click),IsNotNull(none),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1140\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.ContextCleaner: Cleaned accumulator 1138\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 303.1 KB, free 1026.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1026.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.SparkContext: Created broadcast 40 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.SparkContext: Starting job: run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Got job 24 (run at Executors.java:511) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (run at Executors.java:511)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[97] at run at Executors.java:511), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 12.0 KB, free 1026.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1026.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.130.83:42973 (size: 6.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[97] at run at Executors.java:511) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO cluster.YarnScheduler: Adding task set 39.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 76, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8297 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 39.0 (TID 77, algo-1, executor 2, partition 1, PROCESS_LOCAL, 8302 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-2:37179 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:36325 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:28 INFO output.FileOutputCommitter: Saved output of task 'attempt_20250317093526_0031_m_000000_64' to s3a://sagemaker-ap-southeast-1-850995562355/aggregated_clicks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:28 INFO mapred.SparkHadoopMapRedUtil: attempt_20250317093526_0031_m_000000_64: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:28 INFO executor.Executor: Finished task 0.0 in stage 31.0 (TID 64). 5829 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 1.0 in stage 32.0 (TID 66)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 34\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 34 took 18 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO datasources.FileScanRDD: TID: 66 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 14 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 1.0 in stage 32.0 (TID 66). 1595 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 67)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 37\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 37 took 37 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 35\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 35 took 15 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO datasources.FileScanRDD: TID: 67 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 36\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 36 took 22 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 67). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 0.0 in stage 35.0 (TID 69)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Updating epoch to 11 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 2.0 in stage 35.0 (TID 71)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 4.0 in stage 35.0 (TID 73)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 38\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 14.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 38 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 31.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 0.0 in stage 35.0 (TID 69). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 4.0 in stage 35.0 (TID 73). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 2.0 in stage 35.0 (TID 71). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Running task 0.0 in stage 38.0 (TID 75)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Updating epoch to 12 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 39\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 39 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 11, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 3 local blocks and 3 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 76) in 97 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 39.0 (TID 77) in 156 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: ResultStage 39 (run at Executors.java:511) finished in 0.171 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Job 24 finished: run at Executors.java:511, took 0.187346 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 1030.3 KB, free 1025.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 1242.0 B, free 1025.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.130.83:42973 (size: 1242.0 B, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.SparkContext: Created broadcast 42 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 303.1 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.SparkContext: Created broadcast 43 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Registering RDD 100 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Got map stage job 25 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 40 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[100] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 16.0 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 8.3 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.130.83:42973 (size: 8.3 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[100] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO cluster.YarnScheduler: Adding task set 40.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 78, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 40.0 (TID 79, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:36325 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:31 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-2:37179 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-2:37179 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:36325 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 1.0 in stage 35.0 (TID 70). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 5.0 in stage 35.0 (TID 74). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 3.0 in stage 35.0 (TID 72). 3853 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO executor.Executor: Running task 1.0 in stage 39.0 (TID 77)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO spark.MapOutputTrackerWorker: Updating epoch to 12 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 41\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 41 took 18 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO datasources.FileScanRDD: TID: 77 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 40\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 40 took 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO executor.Executor: Finished task 1.0 in stage 39.0 (TID 77). 1595 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO executor.Executor: Running task 0.0 in stage 40.0 (TID 78)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 44\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 44 took 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 42\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 42 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO datasources.FileScanRDD: TID: 78 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 43\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 40.0 (TID 79) in 101 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 78) in 188 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: ShuffleMapStage 40 (count at NativeMethodAccessorImpl.java:0) finished in 0.213 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 351.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO codegen.CodeGenerator: Code generated in 25.997369 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1209\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1280\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1212\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1288\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1246\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1179\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1189\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1219\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1216\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1181\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1277\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1259\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1178\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1180\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1227\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1195\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1182\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1272\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1273\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1193\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1275\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1176\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1228\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1256\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1205\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1206\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1271\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1229\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1247\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1191\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1222\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1188\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1278\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1218\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1262\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1214\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1186\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1204\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1225\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1261\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1185\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1269\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1211\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1224\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1239\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1221\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1223\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1197\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1270\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1274\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1187\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1245\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1233\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1285\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1264\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1290\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1276\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1267\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1254\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1217\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1213\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1279\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1266\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1220\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1283\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1192\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1231\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1282\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1252\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1244\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1237\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1184\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1265\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1253\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1243\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1207\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1235\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1230\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1251\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1177\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1240\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1286\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1291\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1226\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1268\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1250\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1242\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1260\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1234\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1255\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1196\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-2:37179 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.0.130.83:42973 in memory (size: 8.3 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:36325 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1190\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1281\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1249\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1263\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1248\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.130.83:42973 in memory (size: 6.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-2:37179 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:36325 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Registering RDD 105 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Got map stage job 26 (count at NativeMethodAccessorImpl.java:0) with 6 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[105] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1194\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1183\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1215\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1241\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1284\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1238\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.ContextCleaner: Cleaned accumulator 1208\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 31.0 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 14.5 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.130.83:42973 (size: 14.5 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[105] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO cluster.YarnScheduler: Adding task set 42.0 with 6 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 80, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 42.0 (TID 81, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 42.0 (TID 82, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 42.0 (TID 83, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 42.0 (TID 84, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 42.0 (TID 85, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:36325 (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-2:37179 (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 80) in 181 ms on algo-1 (executor 2) (1/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 42.0 (TID 82) in 199 ms on algo-1 (executor 2) (2/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 42.0 (TID 84) in 213 ms on algo-1 (executor 2) (3/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 42.0 (TID 85) in 221 ms on algo-2 (executor 1) (4/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 42.0 (TID 81) in 230 ms on algo-2 (executor 1) (5/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 42.0 (TID 83) in 229 ms on algo-2 (executor 1) (6/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (count at NativeMethodAccessorImpl.java:0) finished in 0.267 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 107.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO codegen.CodeGenerator: Code generated in 41.30155 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Registering RDD 110 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Got map stage job 27 (count at NativeMethodAccessorImpl.java:0) with 13 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 45 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 34.2 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 16.4 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.130.83:42973 (size: 16.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO cluster.YarnScheduler: Adding task set 45.0 with 13 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 86, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 45.0 (TID 87, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 45.0 (TID 88, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 45.0 (TID 89, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 45.0 (TID 90, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 45.0 (TID 91, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 45.0 (TID 92, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 45.0 (TID 93, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:36325 (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-2:37179 (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO codegen.CodeGenerator: Code generated in 10.677744 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:30 INFO executor.Executor: Finished task 0.0 in stage 38.0 (TID 75). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO executor.Executor: Running task 0.0 in stage 39.0 (TID 76)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 41\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 41 took 20 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO datasources.FileScanRDD: TID: 76 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 40\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 40 took 14 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO executor.Executor: Finished task 0.0 in stage 39.0 (TID 76). 1709 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO executor.Executor: Running task 1.0 in stage 40.0 (TID 79)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 44\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 44 took 14 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 45.0 (TID 94, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 45.0 (TID 95, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 45.0 (TID 89) in 109 ms on algo-2 (executor 1) (1/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 45.0 (TID 93) in 110 ms on algo-2 (executor 1) (2/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 45.0 (TID 96, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 45.0 (TID 91) in 119 ms on algo-2 (executor 1) (3/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 45.0 (TID 97, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 45.0 (TID 87) in 127 ms on algo-2 (executor 1) (4/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 45.0 (TID 98, algo-2, executor 1, partition 12, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 45.0 (TID 95) in 37 ms on algo-2 (executor 1) (5/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 45.0 (TID 94) in 42 ms on algo-2 (executor 1) (6/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 45.0 (TID 96) in 28 ms on algo-2 (executor 1) (7/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 45.0 (TID 97) in 32 ms on algo-2 (executor 1) (8/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:32 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 45.0 (TID 98) in 33 ms on algo-2 (executor 1) (9/13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 43 took 29 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 0.0 in stage 40.0 (TID 78). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 0.0 in stage 42.0 (TID 80)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Updating epoch to 13 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 45\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 2.0 in stage 42.0 (TID 82)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 4.0 in stage 42.0 (TID 84)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 14.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 45 took 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 31.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 12, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 12, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 12, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO codegen.CodeGenerator: Code generated in 19.513543 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 0.0 in stage 42.0 (TID 80). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 2.0 in stage 42.0 (TID 82). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 4.0 in stage 42.0 (TID 84). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 0.0 in stage 45.0 (TID 86)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Updating epoch to 14 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 4.0 in stage 45.0 (TID 90)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 46\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 2.0 in stage 45.0 (TID 88)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 6.0 in stage 45.0 (TID 92)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 16.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 86) in 713 ms on algo-1 (executor 2) (10/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 45.0 (TID 92) in 712 ms on algo-1 (executor 2) (11/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 45.0 (TID 90) in 717 ms on algo-1 (executor 2) (12/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 45.0 (TID 88) in 723 ms on algo-1 (executor 2) (13/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (count at NativeMethodAccessorImpl.java:0) finished in 0.736 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1324\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1318\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1315\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1377\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1340\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1378\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1306\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1338\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1298\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1297\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1326\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1320\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1302\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1300\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1336\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1368\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1307\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1365\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1317\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1374\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1327\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1370\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1321\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1301\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1332\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1366\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1375\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1362\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1383\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1323\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1373\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1339\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1319\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1328\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1296\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1337\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1295\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1382\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1314\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1384\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1305\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1380\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1371\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1367\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1303\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1316\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-2:37179 in memory (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.130.83:42973 in memory (size: 14.5 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:36325 in memory (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1363\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1385\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1372\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1376\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.130.83:42973 in memory (size: 16.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-2:37179 in memory (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:36325 in memory (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1293\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1361\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1299\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1369\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1379\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1294\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1333\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1325\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1335\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1331\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1329\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1304\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1322\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1381\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1343\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1330\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1364\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.ContextCleaner: Cleaned accumulator 1334\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO codegen.CodeGenerator: Code generated in 68.913705 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: Got job 28 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 7.6 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 99, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-2:37179 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 99) in 51 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: ResultStage 49 (count at NativeMethodAccessorImpl.java:0) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO scheduler.DAGScheduler: Job 28 finished: count at NativeMethodAccessorImpl.java:0, took 0.069412 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO b[Loggs]grouped_df count 89\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 42\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 42 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO datasources.FileScanRDD: TID: 79 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 43\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 43 took 16 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 1.0 in stage 40.0 (TID 79). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 1.0 in stage 42.0 (TID 81)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 3.0 in stage 42.0 (TID 83)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Updating epoch to 13 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 5.0 in stage 42.0 (TID 85)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 45\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 14.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 45 took 152 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 31.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 12, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 12, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 12, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO codegen.CodeGenerator: Code generated in 17.168458 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 5.0 in stage 42.0 (TID 85). 4560 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 1.0 in stage 42.0 (TID 81). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 3.0 in stage 42.0 (TID 83). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 1.0 in stage 45.0 (TID 87)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 93\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 7.0 in stage 45.0 (TID 93)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 5.0 in stage 45.0 (TID 91)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Updating epoch to 14 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 46\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 3.0 in stage 45.0 (TID 89)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 16.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 46 took 20 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 34.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO codegen.CodeGenerator: Code generated in 25.865919 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 3.0 in stage 45.0 (TID 89). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 7.0 in stage 45.0 (TID 93). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 94\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 95\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 8.0 in stage 45.0 (TID 94)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 9.0 in stage 45.0 (TID 95)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 5.0 in stage 45.0 (TID 91). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 96\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 1.0 in stage 45.0 (TID 87). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 10.0 in stage 45.0 (TID 96)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 97\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 11.0 in stage 45.0 (TID 97)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 9.0 in stage 45.0 (TID 95). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 8.0 in stage 45.0 (TID 94). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 10.0 in stage 45.0 (TID 96). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 98\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Running task 12.0 in stage 45.0 (TID 98)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 11.0 in stage 45.0 (TID 97). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:32 INFO executor.Executor: Finished task 12.0 in stage 45.0 (TID 98). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 99\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO executor.Executor: Running task 0.0 in stage 49.0 (TID 99)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO spark.MapOutputTrackerWorker: Updating epoch to 15 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 47\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 47 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 14, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO storage.ShuffleBlockFetcherIterator: Getting 13 non-empty blocks including 9 local blocks and 4 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO codegen.CodeGenerator: Code generated in 15.22911 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:33 INFO executor.Executor: Finished task 0.0 in stage 49.0 (TID 99). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 1.0 in stage 50.0 (TID 101)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 49\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 49 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO datasources.FileScanRDD: TID: 101 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 48\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 48 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 1.0 in stage 50.0 (TID 101). 1595 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 103\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 1.0 in stage 51.0 (TID 103)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 52\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 52 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 50\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 50 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO datasources.FileScanRDD: TID: 103 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 51\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 51 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 1.0 in stage 51.0 (TID 103). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 104\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 106\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 0.0 in stage 53.0 (TID 104)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 2.0 in stage 53.0 (TID 106)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Updating epoch to 16 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 108\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 4.0 in stage 53.0 (TID 108)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 53\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 14.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 53 took 23 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 31.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 15, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 15, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 15, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 2.0 in stage 53.0 (TID 106). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 4.0 in stage 53.0 (TID 108). 5742 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 0.0 in stage 53.0 (TID 104). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 111\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 113\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 3.0 in stage 56.0 (TID 113)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 1.0 in stage 56.0 (TID 111)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Updating epoch to 17 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 115\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 54\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 117\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 7.0 in stage 56.0 (TID 117)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 5.0 in stage 56.0 (TID 115)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 16.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 54 took 22 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 34.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 7.0 in stage 56.0 (TID 117). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 1.0 in stage 56.0 (TID 111). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 5.0 in stage 56.0 (TID 115). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 3.0 in stage 56.0 (TID 113). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 8.0 in stage 56.0 (TID 118)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 120\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 10.0 in stage 56.0 (TID 120)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 9.0 in stage 56.0 (TID 119)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 121\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 11.0 in stage 56.0 (TID 121)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 10.0 in stage 56.0 (TID 120). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 8.0 in stage 56.0 (TID 118). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 11.0 in stage 56.0 (TID 121). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 9.0 in stage 56.0 (TID 119). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 124\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 0.0 in stage 61.0 (TID 124)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Updating epoch to 18 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 58\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 7.9 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 58 took 11 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 15.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO datasources.FileScanRDD: TID: 124 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 56\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 56 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 126\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 0.0 in stage 62.0 (TID 126)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 59\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 7.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 59 took 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 14.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO datasources.FileScanRDD: TID: 126 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 57\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 57 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 0.0 in stage 61.0 (TID 124). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#170),(event_type#170 = click),isnotnull(customer_id#168L)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: bigint, event_type: string>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click),IsNotNull(customer_id)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:33 INFO execution.FileSourceScanExec: Pushed Filters: EqualTo(none,click),IsNotNull(none),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1345\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1397\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1415\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned shuffle 14\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1351\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1406\u001b[0m\n",
      "\u001b[34mroadcast.TorrentBroadcast: Reading broadcast variable 46 took 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 34.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:32 INFO codegen.CodeGenerator: Code generated in 33.038925 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:33 INFO codegen.CodeGenerator: Code generated in 550.338456 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:33 INFO executor.Executor: Finished task 0.0 in stage 45.0 (TID 86). 4547 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:33 INFO executor.Executor: Finished task 6.0 in stage 45.0 (TID 92). 4547 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1167\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1258\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1159\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1410\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1170\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1360\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1147\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1389\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1422\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1199\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1416\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1142\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1157\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1155\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1137\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1136\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-2:37179 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:36325 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.130.83:42973 in memory (size: 1242.0 B, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1236\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1352\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1391\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1174\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1403\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1348\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1399\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1404\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1346\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1172\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1358\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1405\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1202\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1143\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1171\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1135\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1359\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1139\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1149\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1355\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1413\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1312\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1153\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1310\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1398\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1308\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1168\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1402\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1414\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1401\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned shuffle 13\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1393\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1166\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1289\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1201\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1148\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1169\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1198\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1408\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1309\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1146\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1162\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1141\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1388\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 303.1 KB, free 1026.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1232\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1144\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1165\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1354\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1200\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1163\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1392\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1154\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1150\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1145\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1407\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1161\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1257\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1151\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1390\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1311\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1344\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1357\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1356\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1347\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1313\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1349\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1292\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1173\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on algo-2:37179 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1160\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1353\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1152\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1420\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1158\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned shuffle 12\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1395\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1400\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1341\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1342\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1156\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1287\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1387\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1386\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1175\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1350\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1394\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1164\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1396\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1411\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1409\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1412\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1026.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Created broadcast 48 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Starting job: run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Got job 29 (run at Executors.java:511) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (run at Executors.java:511)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[116] at run at Executors.java:511), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 12.0 KB, free 1026.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1026.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.130.83:42973 (size: 6.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 50 (MapPartitionsRDD[116] at run at Executors.java:511) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO cluster.YarnScheduler: Adding task set 50.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 100, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8297 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 50.0 (TID 101, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8302 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-1:36325 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-2:37179 (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 50.0 (TID 101) in 83 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 100) in 92 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: ResultStage 50 (run at Executors.java:511) finished in 0.103 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Job 29 finished: run at Executors.java:511, took 0.109801 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 1030.3 KB, free 1025.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1242.0 B, free 1025.5 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.130.83:42973 (size: 1242.0 B, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Created broadcast 50 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 303.1 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Created broadcast 51 from count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Registering RDD 119 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Got map stage job 30 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 16.0 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 8.3 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.0.130.83:42973 (size: 8.3 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO cluster.YarnScheduler: Adding task set 51.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 102, algo-1, executor 2, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 51.0 (TID 103, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on algo-2:37179 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on algo-1:36325 (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-2:37179 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-1:36325 (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 51.0 (TID 103) in 91 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 102) in 115 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (count at NativeMethodAccessorImpl.java:0) finished in 0.122 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 351.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1527\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1567\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1491\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1469\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1475\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1573\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1497\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1466\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1534\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1459\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1559\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1458\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1547\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1524\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1471\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1467\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1557\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1503\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1556\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1537\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1519\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1502\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1489\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1558\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1505\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1570\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1509\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1496\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1572\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1562\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1495\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1510\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1513\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1543\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1553\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1490\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1529\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1474\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1555\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1532\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1504\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1520\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1508\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1533\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1546\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.0.130.83:42973 in memory (size: 6.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-2:37179 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-1:36325 in memory (size: 6.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1568\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1512\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1460\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1485\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1530\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1507\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1535\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1487\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1525\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1523\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1554\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1493\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1470\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1564\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1561\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1492\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1566\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1542\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1528\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1468\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1486\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1536\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1560\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1472\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1522\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1498\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1464\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1501\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1549\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1476\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1565\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1544\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1538\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1563\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1526\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1551\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1462\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1531\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1488\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1461\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1478\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1552\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1465\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1506\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1499\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1500\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.0.130.83:42973 in memory (size: 8.3 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on algo-2:37179 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on algo-1:36325 in memory (size: 8.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1479\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1516\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1494\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1541\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1545\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1550\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1517\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1548\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1521\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1477\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1473\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1463\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.ContextCleaner: Cleaned accumulator 1515\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Registering RDD 124 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Got map stage job 31 (count at NativeMethodAccessorImpl.java:0) with 6 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 53 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 53 (MapPartitionsRDD[124] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 31.0 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 14.5 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.0.130.83:42973 (size: 14.5 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 53 (MapPartitionsRDD[124] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO cluster.YarnScheduler: Adding task set 53.0 with 6 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 104, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 53.0 (TID 105, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 53.0 (TID 106, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 53.0 (TID 107, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 53.0 (TID 108, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 53.0 (TID 109, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on algo-2:37179 (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on algo-1:36325 (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 53.0 (TID 106) in 66 ms on algo-2 (executor 1) (1/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 53.0 (TID 108) in 68 ms on algo-2 (executor 1) (2/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 104) in 77 ms on algo-2 (executor 1) (3/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 53.0 (TID 107) in 148 ms on algo-1 (executor 2) (4/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 53.0 (TID 109) in 156 ms on algo-1 (executor 2) (5/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 53.0 (TID 105) in 200 ms on algo-1 (executor 2) (6/6)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: ShuffleMapStage 53 (count at NativeMethodAccessorImpl.java:0) finished in 0.231 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 107.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Registering RDD 129 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Got map stage job 32 (count at NativeMethodAccessorImpl.java:0) with 13 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[129] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 34.2 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 16.4 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.0.130.83:42973 (size: 16.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[129] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO cluster.YarnScheduler: Adding task set 56.0 with 13 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 110, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 56.0 (TID 111, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 56.0 (TID 112, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 56.0 (TID 113, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 56.0 (TID 114, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 56.0 (TID 115, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 56.0 (TID 116, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:34 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 56.0 (TID 117, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on algo-2:37179 (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on algo-1:36325 (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 56.0 (TID 118, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 56.0 (TID 117) in 60 ms on algo-2 (executor 1) (1/13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:33 INFO executor.Executor: Finished task 4.0 in stage 45.0 (TID 90). 4547 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:33 INFO executor.Executor: Finished task 2.0 in stage 45.0 (TID 88). 4547 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 0.0 in stage 50.0 (TID 100)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Updating epoch to 15 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 49\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 6.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 49 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 12.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO datasources.FileScanRDD: TID: 100 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 48\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 48 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 0.0 in stage 50.0 (TID 100). 1709 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 102\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 0.0 in stage 51.0 (TID 102)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 52\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 8.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 52 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 16.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 50\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1242.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 50 took 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 8.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO datasources.FileScanRDD: TID: 102 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream.csv, range: 0-37348, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 51\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 51 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 0.0 in stage 51.0 (TID 102). 3765 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 105\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 107\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 3.0 in stage 53.0 (TID 107)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 1.0 in stage 53.0 (TID 105)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Updating epoch to 16 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 53\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 109\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 5.0 in stage 53.0 (TID 109)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 14.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 53 took 23 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 31.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 15, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 15, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 15, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 3.0 in stage 53.0 (TID 107). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 5.0 in stage 53.0 (TID 109). 4603 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Finished task 1.0 in stage 53.0 (TID 105). 5699 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 110\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.Executor: Running task 0.0 in stage 56.0 (TID 110)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 56.0 (TID 119, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 56.0 (TID 111) in 77 ms on algo-2 (executor 1) (2/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 56.0 (TID 120, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 56.0 (TID 115) in 87 ms on algo-2 (executor 1) (3/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 56.0 (TID 121, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 56.0 (TID 113) in 98 ms on algo-2 (executor 1) (4/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 56.0 (TID 122, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 110) in 116 ms on algo-1 (executor 2) (5/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 56.0 (TID 120) in 58 ms on algo-2 (executor 1) (6/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 56.0 (TID 118) in 90 ms on algo-2 (executor 1) (7/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 56.0 (TID 121) in 66 ms on algo-2 (executor 1) (8/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 56.0 (TID 119) in 97 ms on algo-2 (executor 1) (9/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 56.0 (TID 112) in 184 ms on algo-1 (executor 2) (10/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 56.0 (TID 114) in 193 ms on algo-1 (executor 2) (11/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 56.0 (TID 122) in 123 ms on algo-1 (executor 2) (12/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 56.0 (TID 116) in 239 ms on algo-1 (executor 2) (13/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (count at NativeMethodAccessorImpl.java:0) finished in 0.253 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Got job 33 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Final stage: ResultStage 60 (count at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[132] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 7.6 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1619\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.0.130.83:42973 (size: 4.2 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.0.130.83:42973 in memory (size: 14.5 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on algo-1:36325 in memory (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on algo-2:37179 in memory (size: 14.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1600\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1588\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1655\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1589\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1604\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1643\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1659\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1577\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1644\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1654\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1607\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1583\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[132] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO cluster.YarnScheduler: Adding task set 60.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1606\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1657\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1613\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1581\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1597\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1660\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1608\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1586\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1662\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1649\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1616\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1614\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1651\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1587\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1579\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1663\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1578\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1656\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1658\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1618\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1582\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1609\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1601\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1621\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1620\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1605\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1596\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1645\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1648\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1610\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1615\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1664\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1661\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1652\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1665\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1617\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1653\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1584\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1585\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1576\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1580\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1612\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1611\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1650\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1667\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1625\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1646\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1603\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1602\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1622\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1598\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1666\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 60.0 (TID 123, algo-1, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.0.130.83:42973 in memory (size: 16.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on algo-2:37179 in memory (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on algo-1:36325 in memory (size: 16.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1647\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1575\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1599\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on algo-1:36325 (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 60.0 (TID 123) in 72 ms on algo-1 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO cluster.YarnScheduler: Removed TaskSet 60.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: ResultStage 60 (count at NativeMethodAccessorImpl.java:0) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Job 33 finished: count at NativeMethodAccessorImpl.java:0, took 0.134847 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/c[Loggs]sliced_df count 89\u001b[0m\n",
      "\u001b[34m[Loggs][Transform Spark DataFrame Rows to Feature Store Records]\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO datasources.FileSourceStrategy: Post-Scan Filters: (lower(trim(event_type#30, None)) = purchase)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string, total_amount: double ... 2 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(event_type#170),(event_type#170 = click),isnotnull(customer_id#168L)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO datasources.FileSourceStrategy: Output Data Schema: struct<timestamp: timestamp, customer_id: bigint, event_type: string ... 1 more fields>\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(event_type),EqualTo(event_type,click),IsNotNull(customer_id)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(none),EqualTo(none,click),IsNotNull(none)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 303.1 KB, free 1024.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1624\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1453\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1641\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1025.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.0.130.83:42973 in memory (size: 1242.0 B, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-2:37179 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-1:36325 in memory (size: 1242.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.SparkContext: Created broadcast 56 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1672\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1671\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1452\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1678\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1633\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1593\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1702\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1692\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1706\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1574\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1426\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1636\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1629\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1635\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1446\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1680\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1679\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1437\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1675\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1676\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1436\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1639\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1424\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1693\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1698\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1425\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1455\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1684\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1694\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1640\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned shuffle 16\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1539\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1628\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1418\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Registering RDD 135 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Got map stage job 34 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 61 (javaToPython at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1419\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1540\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1688\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1571\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1595\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1685\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1482\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1428\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1683\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1480\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1483\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1697\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1569\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1431\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1441\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1438\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1435\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1634\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 61 (MapPartitionsRDD[135] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1690\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 303.1 KB, free 1025.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned shuffle 15\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1445\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1642\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1627\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1674\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1484\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1518\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1691\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 15.0 KB, free 1025.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 7.9 KB, free 1025.6 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1025.9 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.0.130.83:42973 (size: 7.9 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.0.130.83:42973 (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 61 (MapPartitionsRDD[135] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO cluster.YarnScheduler: Adding task set 61.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.SparkContext: Created broadcast 57 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 61.0 (TID 124, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 61.0 (TID 125, algo-1, executor 2, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1704\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1632\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1456\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1449\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1432\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1638\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1417\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1686\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1481\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1695\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1668\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1669\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1434\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1590\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1433\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1594\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1696\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1439\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1430\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1687\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1681\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1682\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on algo-1:36325 (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.0.130.83:42973 in memory (size: 27.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on algo-2:37179 (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on algo-2:37179 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on algo-1:36325 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1677\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1448\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1631\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1447\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Registering RDD 138 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 19\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.0.130.83:42973 in memory (size: 4.2 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Got map stage job 35 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 62 (javaToPython at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[138] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on algo-1:36325 in memory (size: 4.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 14.1 KB, free 1026.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1451\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1673\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1623\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1514\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1454\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1442\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1443\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1427\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1592\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1450\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1670\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1630\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1423\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1689\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1591\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1626\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1440\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1637\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned shuffle 17\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1444\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1457\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1429\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.ContextCleaner: Cleaned accumulator 1421\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1026.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.0.130.83:42973 (size: 7.5 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO spark.SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[138] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO cluster.YarnScheduler: Adding task set 62.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 62.0 (TID 126, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8286 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 62.0 (TID 127, algo-1, executor 2, partition 1, PROCESS_LOCAL, 8291 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on algo-2:37179 (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on algo-1:36325 (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on algo-2:37179 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 61.0 (TID 124) in 88 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on algo-1:36325 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 61.0 (TID 125) in 110 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO cluster.YarnScheduler: Removed TaskSet 61.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: ShuffleMapStage 61 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.140 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 62)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 62.0 (TID 126) in 96 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 62.0 (TID 127) in 101 ms on algo-1 (executor 2) (2/2)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO cluster.YarnScheduler: Removed TaskSet 62.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: ShuffleMapStage 62 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.120 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:35 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 129.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.SparkContext: Starting job: run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Got job 36 (run at Executors.java:511) with 16 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Final stage: ResultStage 64 (run at Executors.java:511)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[143] at run at Executors.java:511), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 25.1 KB, free 1026.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 12.1 KB, free 1026.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.0.130.83:42973 (size: 12.1 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ResultStage 64 (MapPartitionsRDD[143] at run at Executors.java:511) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO cluster.YarnScheduler: Adding task set 64.0 with 16 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 64.0 (TID 128, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 64.0 (TID 129, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 64.0 (TID 130, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 64.0 (TID 131, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 64.0 (TID 132, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 64.0 (TID 133, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 64.0 (TID 134, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 64.0 (TID 135, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on algo-2:37179 (size: 12.1 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on algo-1:36325 (size: 12.1 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34montainer_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO spark.MapOutputTrackerWorker: Updating epoch to 17 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 54\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 114\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 116\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 2.0 in stage 56.0 (TID 112)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 4.0 in stage 56.0 (TID 114)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 6.0 in stage 56.0 (TID 116)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 16.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 54 took 36 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 34.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 16, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 0.0 in stage 56.0 (TID 110). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 12.0 in stage 56.0 (TID 122)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 2.0 in stage 56.0 (TID 112). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 4.0 in stage 56.0 (TID 114). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 12.0 in stage 56.0 (TID 122). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 6.0 in stage 56.0 (TID 116). 4504 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 123\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 0.0 in stage 60.0 (TID 123)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Updating epoch to 18 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 55\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 4.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 55 took 17 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 7.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 17, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Getting 13 non-empty blocks including 5 local blocks and 8 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO codegen.CodeGenerator: Code generated in 24.44416 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 0.0 in stage 60.0 (TID 123). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 125\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 1.0 in stage 61.0 (TID 125)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 58\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 7.9 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 58 took 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 15.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO datasources.FileScanRDD: TID: 125 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 56\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 56 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 127\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Running task 1.0 in stage 62.0 (TID 127)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 59\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 7.5 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 59 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 14.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO datasources.FileScanRDD: TID: 127 - Reading current file: path: s3a://sagemaker-ap-southeast-1-850995562355/raw_clicks/clickstream_test.csv, range: 0-29811, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 57\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 57 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 401.8 KB, free 6.2 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 1.0 in stage 61.0 (TID 125). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 1.0 in stage 62.0 (TID 127). 3740 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 128\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 0.0 in stage 64.0 (TID 128)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Updating epoch to 20 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 60\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 130\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 132\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 64.0 (TID 136, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 64.0 (TID 137, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 64.0 (TID 138, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 64.0 (TID 135) in 67 ms on algo-2 (executor 1) (1/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 64.0 (TID 131) in 67 ms on algo-2 (executor 1) (2/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 64.0 (TID 133) in 69 ms on algo-2 (executor 1) (3/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 64.0 (TID 139, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 64.0 (TID 136) in 19 ms on algo-2 (executor 1) (4/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 64.0 (TID 140, algo-2, executor 1, partition 12, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 64.0 (TID 129) in 82 ms on algo-2 (executor 1) (5/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 64.0 (TID 141, algo-2, executor 1, partition 13, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 64.0 (TID 137) in 36 ms on algo-2 (executor 1) (6/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 64.0 (TID 142, algo-2, executor 1, partition 14, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 64.0 (TID 138) in 34 ms on algo-2 (executor 1) (7/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 64.0 (TID 143, algo-2, executor 1, partition 15, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 64.0 (TID 140) in 20 ms on algo-2 (executor 1) (8/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 64.0 (TID 139) in 37 ms on algo-2 (executor 1) (9/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 64.0 (TID 130) in 116 ms on algo-1 (executor 2) (10/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 64.0 (TID 134) in 122 ms on algo-1 (executor 2) (11/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 64.0 (TID 141) in 31 ms on algo-2 (executor 1) (12/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 64.0 (TID 143) in 27 ms on algo-2 (executor 1) (13/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 64.0 (TID 142) in 40 ms on algo-2 (executor 1) (14/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 64.0 (TID 132) in 159 ms on algo-1 (executor 2) (15/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 64.0 (TID 128) in 161 ms on algo-1 (executor 2) (16/16)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO cluster.YarnScheduler: Removed TaskSet 64.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: ResultStage 64 (run at Executors.java:511) finished in 0.170 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Job 36 finished: run at Executors.java:511, took 0.173538 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 1030.3 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1767\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1772\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1806\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.0.130.83:42973 in memory (size: 7.9 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on algo-2:37179 in memory (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 1818.0 B, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on algo-1:36325 in memory (size: 7.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.0.130.83:42973 (size: 1818.0 B, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1740\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1722\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1773\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1725\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1735\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1791\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.SparkContext: Created broadcast 61 from run at Executors.java:511\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1752\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1756\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1780\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1746\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1795\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1733\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1804\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1762\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1738\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1765\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1802\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1808\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1789\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1755\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1753\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1758\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1760\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1726\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1736\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1800\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1801\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1730\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1807\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1794\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1770\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1797\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1790\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1741\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1729\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1805\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1803\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1764\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1751\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1750\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1792\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1710\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1812\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1811\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1737\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1743\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1727\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1759\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1799\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1731\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1763\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1757\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1776\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1809\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1778\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1771\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1761\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1732\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1744\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1774\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1728\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1788\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1781\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.0.130.83:42973 in memory (size: 7.5 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on algo-2:37179 in memory (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on algo-1:36325 in memory (size: 7.5 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1739\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1793\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1768\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1798\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1745\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1749\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1769\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1779\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1723\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1724\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1796\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1754\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1734\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1810\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.0.130.83:42973 in memory (size: 12.1 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 20.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on algo-2:37179 in memory (size: 12.1 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on algo-1:36325 in memory (size: 12.1 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1748\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1742\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1766\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Registering RDD 152 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 20\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Got map stage job 37 (javaToPython at NativeMethodAccessorImpl.java:0) with 15 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 66 (javaToPython at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 65)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 66 (MapPartitionsRDD[152] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 47.6 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1025.2 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.0.130.83:42973 (size: 20.4 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 66 (MapPartitionsRDD[152] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO cluster.YarnScheduler: Adding task set 66.0 with 15 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 66.0 (TID 144, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 66.0 (TID 145, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 66.0 (TID 146, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 66.0 (TID 147, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 66.0 (TID 148, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 66.0 (TID 149, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 66.0 (TID 150, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 66.0 (TID 151, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on algo-2:37179 (size: 20.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on algo-1:36325 (size: 20.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on algo-2:37179 (size: 1818.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 66.0 (TID 152, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 66.0 (TID 149) in 72 ms on algo-2 (executor 1) (1/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 66.0 (TID 153, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 66.0 (TID 151) in 83 ms on algo-2 (executor 1) (2/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 66.0 (TID 154, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 66.0 (TID 155, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 66.0 (TID 145) in 92 ms on algo-2 (executor 1) (3/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 66.0 (TID 147) in 94 ms on algo-2 (executor 1) (4/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 66.0 (TID 156, algo-2, executor 1, partition 12, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 66.0 (TID 153) in 63 ms on algo-2 (executor 1) (5/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 66.0 (TID 157, algo-2, executor 1, partition 13, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1817\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1818\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1820\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO spark.ContextCleaner: Cleaned accumulator 1782\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 66.0 (TID 154) in 107 ms on algo-2 (executor 1) (6/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 66.0 (TID 158, algo-2, executor 1, partition 14, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 66.0 (TID 155) in 107 ms on algo-2 (executor 1) (7/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 66.0 (TID 152) in 127 ms on algo-2 (executor 1) (8/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 66.0 (TID 156) in 55 ms on algo-2 (executor 1) (9/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 66.0 (TID 158) in 49 ms on algo-2 (executor 1) (10/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:36 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 66.0 (TID 157) in 117 ms on algo-2 (executor 1) (11/15)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:35 INFO executor.Executor: Finished task 0.0 in stage 62.0 (TID 126). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 129\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 131\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 3.0 in stage 64.0 (TID 131)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 133\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 135\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 5.0 in stage 64.0 (TID 133)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Updating epoch to 20 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 60\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 7.0 in stage 64.0 (TID 135)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 1.0 in stage 64.0 (TID 129)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 12.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Reading broadcast variable 60 took 16 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 25.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 5.0 in stage 64.0 (TID 133). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 3.0 in stage 64.0 (TID 131). 2718 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 8.0 in stage 64.0 (TID 136)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 137\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 7.0 in stage 64.0 (TID 135). 2718 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 9.0 in stage 64.0 (TID 137)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 138\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 10.0 in stage 64.0 (TID 138)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 8.0 in stage 64.0 (TID 136). 2746 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 1.0 in stage 64.0 (TID 129). 2746 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 139\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 11.0 in stage 64.0 (TID 139)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 140\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 12.0 in stage 64.0 (TID 140)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 9.0 in stage 64.0 (TID 137). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 10.0 in stage 64.0 (TID 138). 2738 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 12.0 in stage 64.0 (TID 140). 2766 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 141\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 13.0 in stage 64.0 (TID 141)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 142\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 14.0 in stage 64.0 (TID 142)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 11.0 in stage 64.0 (TID 139). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 13.0 in stage 64.0 (TID 141). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 143\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 15.0 in stage 64.0 (TID 143)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 15.0 in stage 64.0 (TID 143). 2677 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 14.0 in stage 64.0 (TID 142). 3169 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 145\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 147\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 1.0 in stage 66.0 (TID 145)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 3.0 in stage 66.0 (TID 147)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 149\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 5.0 in stage 66.0 (TID 149)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 151\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 7.0 in stage 66.0 (TID 151)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 62\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 20.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Reading broadcast variable 62 took 12 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 47.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 18, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 18, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 18, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 18, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 61\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 1818.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Reading broadcast variable 61 took 12 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 9.7 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 5.0 in stage 66.0 (TID 149). 7062 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 7.0 in stage 66.0 (TID 151). 7062 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 3.0 in stage 66.0 (TID 147). 7062 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 1.0 in stage 66.0 (TID 145). 7062 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 152\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 8.0 in stage 66.0 (TID 152)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 153\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 9.0 in stage 66.0 (TID 153)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 154\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 10.0 in stage 66.0 (TID 154)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 155\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 11.0 in stage 66.0 (TID 155)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 9.0 in stage 66.0 (TID 153). 7062 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 156\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 12.0 in stage 66.0 (TID 156)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 10.0 in stage 66.0 (TID 154). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 11.0 in stage 66.0 (TID 155). 7062 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 8.0 in stage 66.0 (TID 152). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 12.0 in stage 66.0 (TID 156). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 157\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 13.0 in stage 66.0 (TID 157)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 14.0 in stage 66.0 (TID 158)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 14.0 in stage 66.0 (TID 158). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on algo-1:36325 (size: 1818.0 B, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 2.0 in stage 64.0 (TID 130)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 134\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 4.0 in stage 64.0 (TID 132)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 6.0 in stage 64.0 (TID 134)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 12.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Reading broadcast variable 60 took 19 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 25.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 19, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 6.0 in stage 64.0 (TID 134). 2745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 2.0 in stage 64.0 (TID 130). 2771 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 4.0 in stage 64.0 (TID 132). 2746 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 0.0 in stage 64.0 (TID 128). 2726 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 144\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 0.0 in stage 66.0 (TID 144)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 146\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 148\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 62\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 2.0 in stage 66.0 (TID 146)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 4.0 in stage 66.0 (TID 148)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO executor.Executor: Running task 6.0 in stage 66.0 (TID 150)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 20.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO broadcast.TorrentBroadcast: Reading broadcast variable 62 took 17 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 47.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 18, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 18, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 16.809925 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 10.458632 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 16.234202 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 11.597123 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 13.716863 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 10.145833 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 31.744303 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 19.63656 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 13.139488 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 16.55199 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 26.286994 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 8.847767 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 66.0 (TID 150) in 624 ms on algo-1 (executor 2) (12/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 66.0 (TID 144) in 626 ms on algo-1 (executor 2) (13/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 66.0 (TID 146) in 627 ms on algo-1 (executor 2) (14/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 66.0 (TID 148) in 628 ms on algo-1 (executor 2) (15/15)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO cluster.YarnScheduler: Removed TaskSet 66.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: ShuffleMapStage 66 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.641 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 196.\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 25.287759 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 16.988291 ms\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO spark.SparkContext: Starting job: collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:145\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: Got job 38 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:145) with 13 output partitions\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: Final stage: ResultStage 69 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:145)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 68)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[159] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 43.0 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 20.0 KB, free 1025.1 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.0.130.83:42973 (size: 20.0 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO spark.SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: Submitting 13 missing tasks from ResultStage 69 (MapPartitionsRDD[159] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO cluster.YarnScheduler: Adding task set 69.0 with 13 tasks\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 69.0 (TID 159, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 69.0 (TID 160, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 69.0 (TID 161, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 69.0 (TID 162, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 69.0 (TID 163, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 69.0 (TID 164, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 69.0 (TID 165, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 69.0 (TID 166, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on algo-2:37179 (size: 20.0 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on algo-1:36325 (size: 20.0 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.0.130.83:58014\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.0.164.134:53210\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 69.0 (TID 167, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 69.0 (TID 159) in 241 ms on algo-1 (executor 2) (1/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 69.0 (TID 168, algo-1, executor 2, partition 9, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 69.0 (TID 161) in 247 ms on algo-1 (executor 2) (2/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 69.0 (TID 169, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 69.0 (TID 163) in 255 ms on algo-1 (executor 2) (3/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 69.0 (TID 170, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 69.0 (TID 167) in 52 ms on algo-1 (executor 2) (4/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 69.0 (TID 171, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 69.0 (TID 169) in 47 ms on algo-1 (executor 2) (5/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 69.0 (TID 160) in 321 ms on algo-2 (executor 1) (6/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 69.0 (TID 164) in 320 ms on algo-2 (executor 1) (7/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 69.0 (TID 162) in 322 ms on algo-2 (executor 1) (8/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 69.0 (TID 166) in 322 ms on algo-2 (executor 1) (9/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 69.0 (TID 171) in 60 ms on algo-1 (executor 2) (10/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 69.0 (TID 168) in 241 ms on algo-1 (executor 2) (11/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 69.0 (TID 165) in 489 ms on algo-1 (executor 2) (12/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 69.0 (TID 170) in 206 ms on algo-1 (executor 2) (13/13)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO cluster.YarnScheduler: Removed TaskSet 69.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: ResultStage 69 (collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:145) finished in 0.513 s\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:37 INFO scheduler.DAGScheduler: Job 38 finished: collect at /opt/ml/processing/input/code/batch_aggregation_user_behavior.py:145, took 0.517126 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 IN[Loggs][Write Grouped Features to SageMaker Feature Store] records count- 89\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 1, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 2, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 3, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 4, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 5, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 6, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 7, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 8, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 9, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 10, Failed = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:36 INFO executor.Executor: Finished task 13.0 in stage 66.0 (TID 157). 7062 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 160\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 1.0 in stage 69.0 (TID 160)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 162\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 3.0 in stage 69.0 (TID 162)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Updating epoch to 21 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 164\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 166\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 5.0 in stage 69.0 (TID 164)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 7.0 in stage 69.0 (TID 166)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 63\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 20.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 63 took 18 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 43.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 11, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 12, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 13, Failed = 0\u001b[0m\n",
      "\u001b[34mFO codegen.CodeGenerator: Code generated in 13.304626 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 6.960393 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 20.19669 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:36 INFO codegen.CodeGenerator: Code generated in 15.221293 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 37.336179 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 61\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 1818.0 B, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 61 took 24 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 9.7 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 14.314755 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 26.305932 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Finished task 6.0 in stage 66.0 (TID 150). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Finished task 0.0 in stage 66.0 (TID 144). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Finished task 2.0 in stage 66.0 (TID 146). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Finished task 4.0 in stage 66.0 (TID 148). 5923 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 161\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 163\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 165\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 0.0 in stage 69.0 (TID 159)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 4.0 in stage 69.0 (TID 163)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Updating epoch to 21 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 6.0 in stage 69.0 (TID 165)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 63\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 2.0 in stage 69.0 (TID 161)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 20.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 63 took 18 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 43.0 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.130.83:46483)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 20, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 31.267528 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 9.41291 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 11.928487 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 29.415646 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO codegen.CodeGenerator: Code generated in 9.216941 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Finished task 0.0 in stage 69.0 (TID 159). 5406 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Finished task 2.0 in stage 69.0 (TID 161). 5406 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Finished task 4.0 in stage 69.0 (TID 163). 5406 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 167\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 8.0 in stage 69.0 (TID 167)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 168\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 169\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 10.0 in stage 69.0 (TID 169)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO executor.Executor: Running task 9.0 in stage 69.0 (TID 168)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 0 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000003/stderr] 25/03/17 09:35:37 INFO storage.ShuffleBlockFetcherIterator[Loggs]Success = 14, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 15, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 16, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 17, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 18, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 19, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 20, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 21, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 22, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 23, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 24, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 25, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 26, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 27, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 28, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 29, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 30, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 31, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 32, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 33, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 34, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 35, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 36, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 37, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 38, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 39, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 40, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 41, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 42, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 43, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 44, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 45, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 46, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 47, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 48, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 49, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 50, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 51, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 52, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 53, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 54, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 55, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 56, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 57, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 58, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 59, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 60, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 61, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 62, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 63, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 64, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 65, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 66, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 67, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 68, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 69, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 70, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 71, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 72, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 73, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 74, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 75, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 76, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 77, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 78, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 79, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 80, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 81, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 82, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 83, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 84, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 85, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 86, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 87, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 88, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 89, Failed = 0\u001b[0m\n",
      "\u001b[34m[Loggs]Success = 89\u001b[0m\n",
      "\u001b[34m[Loggs]Fail = 0\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO launcher.ContainerLaunch: Container container_1742204047158_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO launcher.ContainerLaunch: Cleaning up container container_1742204047158_0001_01_000002\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000002\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000002\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO application.ApplicationImpl: Removing container_1742204047158_0001_01_000002 from application application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1742204047158_0001_01_000002\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:39 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO server.AbstractConnector: Stopped Spark@1f412009{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.130.83:4040\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7606e37b-d1be-455f-8687-46fa277f10a6\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-17896d4d-e877-48b3-9dfc-44cad2f3322d\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7606e37b-d1be-455f-8687-46fa277f10a6/pyspark-47fbd548-b8e0-478d-a8ee-8b5bf547efdf\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1742204047158_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO rmapp.RMAppImpl: Updating application application_1742204047158_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO recovery.RMStateStore: Updating info for app: application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO rmapp.RMAppImpl: application_1742204047158_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO rmapp.RMAppImpl: application_1742204047158_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO launcher.ContainerLaunch: Container container_1742204047158_0001_01_000003 succeeded \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO launcher.ContainerLaunch: Cleaning up container container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO application.ApplicationImpl: Removing container_1742204047158_0001_01_000003 from application application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1742204047158_0001_01_000003\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000003 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO resourcemanager.ApplicationMasterService: application_1742204047158_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO namenode.FSEditLog: Number of transactions: 44 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 12 Number of syncs: 32 SyncTimes(ms): 58 \u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:39 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m03-17 09:35 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO launcher.ContainerLaunch: Container container_1742204047158_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO launcher.ContainerLaunch: Cleaning up container container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001/container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO container.ContainerImpl: Container container_1742204047158_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO application.ApplicationImpl: Removing container_1742204047158_0001_01_000001 from application application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO ipc.Server: Auth successful for appattempt_1742204047158_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:40 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.130.83#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO rmcontainer.RMContainerImpl: container_1742204047158_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1742204047158_0001#011CONTAINERID=container_1742204047158_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO attempt.RMAppAttemptImpl: appattempt_1742204047158_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO rmapp.RMAppImpl: application_1742204047158_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO capacity.CapacityScheduler: Application Attempt appattempt_1742204047158_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO scheduler.AppSchedulingInfo: Application application_1742204047158_0001 requests cleared\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO capacity.LeafQueue: Application removed - appId: application_1742204047158_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO capacity.ParentQueue: Application removed - appId: application_1742204047158_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO amlauncher.AMLauncher: Cleaning master appattempt_1742204047158_0001_000001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1742204047158_0001,name=UserBehaviorBatchAggregationJob5,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1742204047158_0001/,appMasterHost=10.0.164.134,submitTime=1742204074514,startTime=1742204074626,launchTime=1742204076310,finishTime=1742204139562,finalStatus=SUCCEEDED,memorySeconds=1559610,vcoreSeconds=173,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=1559610 MB-seconds\\, 173 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=,applicationNodeLabel=\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:40 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1742204047158_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:41 INFO nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1742204047158_0001_01_000001]\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:41 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:41 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:41 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:41 INFO application.ApplicationImpl: Application application_1742204047158_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:41 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1742204047158_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[34m25/03/17 09:35:42 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:44 INFO impl.FsDatasetAsyncDiskService: Deleted BP-128958768-10.0.130.83-1742204036289 blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-128958768-10.0.130.83-1742204036289/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:47 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"algo-2/10.0.164.134\"; destination host is: \"algo-1\":8020; \u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:808)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1544)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1486)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1385)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:516)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:646)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:846)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35mCaused by: java.io.IOException: Connection reset by peer\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.FileDispatcherImpl.read0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.IOUtil.read(IOUtil.java:197)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\u001b[0m\n",
      "\u001b[35m#011at java.io.FilterInputStream.read(FilterInputStream.java:133)\u001b[0m\n",
      "\u001b[35m#011at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\u001b[0m\n",
      "\u001b[35m#011at java.io.BufferedInputStream.read(BufferedInputStream.java:265)\u001b[0m\n",
      "\u001b[35m#011at java.io.FilterInputStream.read(FilterInputStream.java:83)\u001b[0m\n",
      "\u001b[35m#011at java.io.FilterInputStream.read(FilterInputStream.java:83)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:569)\u001b[0m\n",
      "\u001b[35m#011at java.io.DataInputStream.readInt(DataInputStream.java:387)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1845)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1184)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1080)\u001b[0m\n",
      "\u001b[34m03-17 09:35 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1742204047158_0001\u001b[0m\n",
      "\u001b[34m03-17 09:35 root         INFO     copying /tmp/spark-events/application_1742204047158_0001 to /opt/ml/processing/spark-events/application_1742204047158_0001\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:48 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.164.134\"; destination host is: \"algo-1\":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.\u001b[0m\n",
      "\u001b[35m03-17 09:35 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd27f746c10>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m03-17 09:35 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd27f7468d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:51 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m03-17 09:35 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd27f746ed0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:52 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:53 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:54 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:55 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m03-17 09:35 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd27f72f0d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:56 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:57 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:58 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:35:59 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:36:00 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:36:00 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.net.ConnectException: Call From algo-2/10.0.164.134 to algo-1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:824)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:754)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1544)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1486)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1385)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:516)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:646)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:846)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35mCaused by: java.net.ConnectException: Connection refused\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:701)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:805)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getConnection(Client.java:1601)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1432)\u001b[0m\n",
      "\u001b[35m#011... 9 more\u001b[0m\n",
      "\u001b[35m25/03/17 09:36:02 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m25/03/17 09:36:03 INFO ipc.Client: Retrying connect to server: algo-1/10.0.130.83:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m03-17 09:36 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd27f72f610>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m03-17 09:36 smspark-submit INFO     primary is down, worker now exiting\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1742204047158_0001/container_1742204047158_0001_01_000002/stderr] 25/03/17 09:35:37 INFO st\u001b[0m\n",
      "\n",
      "CPU times: user 1.02 s, sys: 63.3 ms, total: 1.09 s\n",
      "Wall time: 7min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark_processor.run(submit_app='batch_aggregation_user_behavior.py', \n",
    "                    arguments=['--s3_input_bucket', BUCKET, \n",
    "                               '--s3_input_key_prefix', INPUT_KEY_PREFIX, \n",
    "                               '--s3_output_bucket', BUCKET, \n",
    "                               '--s3_output_key_prefix', OUTPUT_KEY_PREFIX],\n",
    "                    spark_event_logs_s3_uri='s3://{}/logs'.format(BUCKET),\n",
    "                    logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Explore Aggregated Features \n",
    "<p> The SageMaker Processing Job above creates the aggregated features alongside the raw features and writes it to S3. \n",
    "Let us verify this output using the code below and prep it to be used in the next step for model training.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Copy results csv from S3 to local directory. Below step may fail if you are running the notebook for the first time. Hence any file missing error could be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/home/sagemaker-user/cautious-parakeet/notebooks/data/aggregated_clicks/part*.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-ap-southeast-1-850995562355/aggregated_clicks/part-00000-ae1a25ad-40fc-4868-8adf-c0f3fcd28d17-c000.csv to data/aggregated_clicks/part-00000-ae1a25ad-40fc-4868-8adf-c0f3fcd28d17-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://{BUCKET}/{OUTPUT_KEY_PREFIX}/ {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/ --recursive --exclude '_SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp s3://sagemaker-ap-southeast-1-850995562355/aggregated/ {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/cc/ --recursive --exclude '_SUCCESS'\n",
    "# !mv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/cc/part*.csv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/partcc.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7492/1654825752.py:8: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  agg_features['event_time'] = agg_features['event_time'].values.astype('datetime64[ns]')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>total_orders_last_1w</th>\n",
       "      <th>avg_order_value_last_1w</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.350006</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.350006</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.350006</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.350006</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>493.290009</td>\n",
       "      <td>2025-03-01 02:04:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  total_orders_last_1w  avg_order_value_last_1w          event_time\n",
       "0       25                     1               446.350006 2025-03-01 02:03:40\n",
       "1       25                     1               446.350006 2025-03-01 02:03:40\n",
       "2       25                     1               446.350006 2025-03-01 02:03:40\n",
       "3       25                     1               446.350006 2025-03-01 02:03:40\n",
       "4      176                     1               493.290009 2025-03-01 02:04:59"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_features = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv')\n",
    "# agg_features = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/partcc.csv')\n",
    "agg_features.dropna(inplace=True)\n",
    "agg_features = agg_features.drop(columns = [\"clicks_last_5m\"])\n",
    "agg_features['total_orders_last_1w'] = agg_features['total_orders_last_1w'].astype(np.int64)\n",
    "agg_features['avg_order_value_last_1w'] = agg_features['avg_order_value_last_1w'].astype(np.float32)\n",
    "# agg_features['event_time'] = agg_features['event_time'].astype(np.datetime64)\n",
    "agg_features['event_time'] = agg_features['event_time'].values.astype('datetime64[ns]')\n",
    "agg_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_features.to_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Remove the intermediate `part.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Validate Feature Group for Records\n",
    "Let's randomly pick N credit card numbers from the `processing_output.csv` and verify if records exist in the feature group `<aggregate_batch_feature_group_name>` for these card numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 3 # number of random records to validate\n",
    "FEATURE_GROUP = aggregate_batch_feature_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>total_orders_last_1w</th>\n",
       "      <th>avg_order_value_last_1w</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.35</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.35</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.35</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>446.35</td>\n",
       "      <td>2025-03-01 02:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>493.29</td>\n",
       "      <td>2025-03-01 02:04:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  total_orders_last_1w  avg_order_value_last_1w           event_time\n",
       "0       25                     1                   446.35  2025-03-01 02:03:40\n",
       "1       25                     1                   446.35  2025-03-01 02:03:40\n",
       "2       25                     1                   446.35  2025-03-01 02:03:40\n",
       "3       25                     1                   446.35  2025-03-01 02:03:40\n",
       "4      176                     1                   493.29  2025-03-01 02:04:59"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_out_df = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv', nrows=10000)\n",
    "processing_out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[660, 448, 139]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_nums = random.sample(processing_out_df['user_id'].tolist(), N)\n",
    "cc_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Using SageMaker Feature Store run-time client, we can verify if records exist in the feature group for the picked `cc_nums` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_store_client = boto3.Session().client(service_name='sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FeatureName': 'user_id', 'ValueAsString': '660'}, {'FeatureName': 'total_orders_last_1w', 'ValueAsString': '1'}, {'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': '384.13'}, {'FeatureName': 'event_time', 'ValueAsString': '1742204138'}]\n",
      "[{'FeatureName': 'user_id', 'ValueAsString': '448'}, {'FeatureName': 'total_orders_last_1w', 'ValueAsString': '1'}, {'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': '376.75'}, {'FeatureName': 'event_time', 'ValueAsString': '1742204138'}]\n",
      "[{'FeatureName': 'user_id', 'ValueAsString': '139'}, {'FeatureName': 'total_orders_last_1w', 'ValueAsString': '1'}, {'FeatureName': 'avg_order_value_last_1w', 'ValueAsString': '425.38'}, {'FeatureName': 'event_time', 'ValueAsString': '1742204138'}]\n"
     ]
    }
   ],
   "source": [
    "success, fail = 0, 0\n",
    "for cc_num in cc_nums:\n",
    "    response = feature_store_client.get_record(FeatureGroupName=FEATURE_GROUP, \n",
    "                                               RecordIdentifierValueAsString=str(cc_num))\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] == 200 and 'Record' in response.keys():\n",
    "        success += 1\n",
    "        print(response['Record'])\n",
    "    else:\n",
    "        print(response)\n",
    "        fail += 1\n",
    "assert success == N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
